{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "doc2vec PV-DM Distributed Memory version of Paragraph Vector.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP8ZAzyYrnNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c8ae80fb-096f-4632-f465-6a7aa30350ec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOPCCBOprNa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hfXqWxwrNbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37ca4835-15fb-471e-bbf3-39e6b77ec296"
      },
      "source": [
        "import sys  \n",
        "from imp import reload\n",
        "\n",
        "reload(sys)  \n",
        "#sys.setdefaultencoding('utf8')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'sys' (built-in)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-S6xHNnrNbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebb591f1-dc30-45fd-f2b2-c708328f3007"
      },
      "source": [
        "import sys\n",
        "reload(sys)  \n",
        "#sys.setdefaultencoding('Cp1252')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'sys' (built-in)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lym3QpuorNbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mpld3\n",
        "mpld3.enable_notebook()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4qaR3hQrNbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('../../Utils/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTUaLAD8rNbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 10, 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHO7CNd4rNbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk\n",
        "import math\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import shuffle\n",
        "#from load_imdb_data import load_imdb_data\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akDpDLTorNbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open(\"/content/drive/NLP_bootcamp/Data/intent_data.txt\").readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx2HI2g6rNbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd2TgU1arNb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = []\n",
        "labels = []\n",
        "\n",
        "for item in data:\n",
        "    temp = item.split('   ')\n",
        "    sentence.append(temp[0].strip())\n",
        "    labels.append(temp[1].strip().replace(\"\\n\", ''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcP_1HaLrNb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentence = [line.decode('utf-8').strip() for line in sentence]\n",
        "sentence = [line.strip() for line in sentence]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llJHvVBkrNb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ee2a143d-cc71-4b26-bff4-4b1d6c56d481"
      },
      "source": [
        "sentence[:2], labels[:2]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['I must look like a right weirdo .',\n",
              "  \"I really don't want to get up and get dressed for work\"],\n",
              " ['non-intent', 'trifle'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcY6HrrTrNcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['review'] = sentence\n",
        "df['label'] = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHdRVyGzrNcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df['label'].isin(['food', 'travel', 'career'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2URnjxnqrNcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtaXC87NrNcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5f5113a5-ae7c-4574-a5d3-186b7a847dd9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I really want a hot dog . My co-workers went t...</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I wanna go to the beach , but ... Where is the...</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really look forward to Egypt next week ! Its...</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I should buy more oatmeal cookies .. I just at...</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I want pancakes , should I make some ???</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review   label\n",
              "0  I really want a hot dog . My co-workers went t...    food\n",
              "1  I wanna go to the beach , but ... Where is the...  travel\n",
              "2  I really look forward to Egypt next week ! Its...  travel\n",
              "3  I should buy more oatmeal cookies .. I just at...    food\n",
              "4           I want pancakes , should I make some ???    food"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEi3VFJDrNcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processDocs(documents, vocab_size=5000):\n",
        "    \"\"\"\n",
        "    This functions takes in a collection of documents and generates a vocabulary based on the size given in input. \n",
        "    It returns a representation for each document in the list of input documents. \n",
        "    \"\"\"\n",
        "    vocab = {} \n",
        "    doc_id = 0 \n",
        "    doc_ids = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        doc_ids.append(doc_id)                          # Give an ID to each document \n",
        "        doc_id += 1\n",
        "        \n",
        "        for word in nltk.word_tokenize(doc):            # Generate a vocabulary while iterating threw the documents \n",
        "            if word not in vocab:\n",
        "                vocab[word] = 1 \n",
        "            else:\n",
        "                vocab[word] += 1\n",
        "    \n",
        "    # Extract the most frequent words based on the vocabulary size \n",
        "    freq_words_list = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:vocab_size]\n",
        "    freq_words_set = set([item[0] for item in freq_words_list])\n",
        "    \n",
        "    # Give an index to each word in vocabulary \n",
        "    word2idx = {}         \n",
        "    index_word = 0\n",
        "    for word in freq_words_set:\n",
        "        word2idx[word] = index_word\n",
        "        index_word += 1\n",
        "    word2idx['UNK'] = index_word\n",
        "    \n",
        "    doc_repr = []                          # Represent each document with representation based on the vocabulary  \n",
        "    for doc in documents:\n",
        "        temp = []\n",
        "        for w in doc:\n",
        "            if w in word2idx:\n",
        "                temp.append(word2idx[w])\n",
        "            else:\n",
        "                temp.append(word2idx['UNK'])\n",
        "        doc_repr.append(temp)\n",
        "        \n",
        "    return documents, doc_ids, word2idx, doc_repr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOX5cVW3rNcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs, doc_ids, word2ids, doc_repr = processDocs(df['review'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUl41GirNcZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a2e5bfe-166b-45f7-eb9e-10e00974884c"
      },
      "source": [
        "len(docs), len(doc_ids), len(word2ids), len(doc_repr)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(591, 591, 1900, 591)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TftSKUXsrNcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTPD7i9QrNch",
        "colab_type": "text"
      },
      "source": [
        "## Architecture - PV-DM Distributed Memory version of Paragraph Vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hn2tIE0ArNcm",
        "colab_type": "text"
      },
      "source": [
        "### Adding batchsizes for speedup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7qvVgqzrNcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bucket_list = []\n",
        "\n",
        "def generate_batch_pvdm(doc_ids, doc_repr, sample_size=10, batch_size=1000, window_size=7):\n",
        "    global bucket_list\n",
        "\n",
        "    docs_ids_to_select = list(set(doc_ids) - set(bucket_list))\n",
        "    \n",
        "    \n",
        "    if len(docs_ids_to_select) < batch_size//sample_size:\n",
        "        bucket_list = []\n",
        "        docs_ids_to_select = doc_ids\n",
        "        \n",
        "    index = 0 \n",
        "    train_wX = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
        "    train_dX = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    random_docs = random.sample(docs_ids_to_select, batch_size//sample_size)    # Choose set of random documents \n",
        "\n",
        "    bucket_list += random_docs\n",
        "    \n",
        "    for id_ in random_docs:\n",
        "        for j in range(sample_size):                                 # Generating a dataset of sample size \n",
        "            random_index = random.randint(0, len(doc_repr[id_]) - window_size - 1)\n",
        "            sample_window = doc_repr[id_][random_index: random_index + window_size + 1]\n",
        "            train_wX[index] = sample_window[:-1]\n",
        "            train_dX[index] = id_\n",
        "            train_label[index] = sample_window[-1]  \n",
        "            index += 1\n",
        "    return train_wX, train_dX, train_label "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypiGq4ENrNct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_size = len(docs)\n",
        "embedding_size_w = 100\n",
        "embedding_size_d = 100\n",
        "vocab_size = len(word2ids)\n",
        "window_size = 7\n",
        "n_neg_samples = 30\n",
        "learning_rate = 10e-6\n",
        "epochs = 10001\n",
        "batch_size=1000\n",
        "mu=0.9\n",
        "combined_embed_vector_length = embedding_size_d + embedding_size_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqiv4HxrNcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define placeholders for training \n",
        "train_wX = tf.placeholder(tf.int32, shape=[batch_size, window_size])\n",
        "train_dX = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "train_label = tf.placeholder(tf.int32, shape=[batch_size, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dkUIlv5rNcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_embedding_np = np.random.randn(doc_size, embedding_size_d)/np.sqrt(doc_size + embedding_size_d)\n",
        "word_embedding_np = np.random.randn(vocab_size, embedding_size_w)/np.sqrt(vocab_size + embedding_size_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe0WARwWrNc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define matrix for doc_embedding and word_embedding \n",
        "doc_embedding = tf.Variable(doc_embedding_np.astype(np.float32), name=\"doc_embedding\")\n",
        "word_embedding = tf.Variable(word_embedding_np.astype(np.float32),name=\"word_embedding\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbTqjNt1rNc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define weights for the output unit \n",
        "weights = tf.Variable(tf.truncated_normal([vocab_size, combined_embed_vector_length], \n",
        "                                       stddev=1.0 / math.sqrt(combined_embed_vector_length)))\n",
        "biases = tf.Variable(tf.zeros(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K08mqyFPrNc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8e7b565-2218-4c43-b346-57c92fc4e0e5"
      },
      "source": [
        "embed = []\n",
        "\n",
        "# generating a vector of size embedding_size_d\n",
        "embed_w = tf.zeros([1, embedding_size_w], dtype=tf.float32)\n",
        "\n",
        "# add all the word vecs in window_size\n",
        "for j in range(window_size):\n",
        "    embed_w += tf.nn.embedding_lookup(word_embedding, train_wX[:, j])\n",
        "embed.append(embed_w)\n",
        "\n",
        "# Add the doc2vec from the doc_embedding \n",
        "embed_d = tf.nn.embedding_lookup(doc_embedding, train_dX[:, 0])\n",
        "embed.append(embed_d)\n",
        "\n",
        "print (embed_w, embed_d)\n",
        "\n",
        "embed = tf.concat(embed, 1)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"add_6:0\", shape=(1000, 100), dtype=float32) Tensor(\"embedding_lookup_7/Identity:0\", shape=(1000, 100), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9jiQWNsrNdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.nn.sampled_softmax_loss(weights=weights, \\\n",
        "                                  biases=biases, \\\n",
        "                                  labels=train_label, \\\n",
        "                                  inputs=embed, \\\n",
        "                                  num_sampled=n_neg_samples, \\\n",
        "                                  num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsA0lpXlrNdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqk1JPEurNdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "667442c6-504d-45b7-a835-121ba86675b9"
      },
      "source": [
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Siq3KDrNdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2f0b9d8-be95-49ab-d25f-788a31f03858"
      },
      "source": [
        "import os\n",
        "os.system(\"mkdir /dev/shm/tensorflow_models\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hVlglyS-rNda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "4ec03f15-76ad-4cf3-8286-f48ac29b53e8"
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    average_loss = 0\n",
        "    \n",
        "    for step in range(epochs):\n",
        "        epoch_error = 0.0\n",
        "        temp_wX , temp_dX, temp_labels = generate_batch_pvdm(doc_ids=doc_ids, doc_repr=doc_repr)\n",
        "        feed_dict = {train_wX : temp_wX, train_dX : temp_dX,train_label : temp_labels}\n",
        "        op, l = sess.run([optimizer, loss], \n",
        "                                    feed_dict=feed_dict)\n",
        "        \n",
        "        epoch_error += l\n",
        "                \n",
        "        if step % 100 == 0:\n",
        "            print (\"Error at epoch : \", step, \" = \", epoch_error)\n",
        "            \n",
        "    save_path = saver.save(sess, \"/dev/shm/tensorflow_models/model_pvdm_batch_training.ckpt\")\n",
        "    print(\"Model saved in file: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error at epoch :  0  =  1.7232497930526733\n",
            "Error at epoch :  100  =  2.2426693439483643\n",
            "Error at epoch :  200  =  2.0143673419952393\n",
            "Error at epoch :  300  =  2.290821075439453\n",
            "Error at epoch :  400  =  2.0192670822143555\n",
            "Error at epoch :  500  =  2.1034131050109863\n",
            "Error at epoch :  600  =  2.268862247467041\n",
            "Error at epoch :  700  =  1.9372295141220093\n",
            "Error at epoch :  800  =  2.182295799255371\n",
            "Error at epoch :  900  =  1.7573515176773071\n",
            "Error at epoch :  1000  =  2.4906678199768066\n",
            "Error at epoch :  1100  =  2.3477845191955566\n",
            "Error at epoch :  1200  =  2.4751057624816895\n",
            "Error at epoch :  1300  =  2.1769134998321533\n",
            "Error at epoch :  1400  =  2.1097893714904785\n",
            "Error at epoch :  1500  =  1.9440593719482422\n",
            "Error at epoch :  1600  =  2.2980892658233643\n",
            "Error at epoch :  1700  =  2.4752182960510254\n",
            "Error at epoch :  1800  =  1.9294853210449219\n",
            "Error at epoch :  1900  =  2.4027278423309326\n",
            "Error at epoch :  2000  =  2.095219373703003\n",
            "Error at epoch :  2100  =  2.527587413787842\n",
            "Error at epoch :  2200  =  1.7921476364135742\n",
            "Error at epoch :  2300  =  2.101208448410034\n",
            "Error at epoch :  2400  =  1.9997332096099854\n",
            "Error at epoch :  2500  =  1.8166325092315674\n",
            "Error at epoch :  2600  =  2.3318519592285156\n",
            "Error at epoch :  2700  =  1.9742443561553955\n",
            "Error at epoch :  2800  =  2.0683133602142334\n",
            "Error at epoch :  2900  =  1.9007078409194946\n",
            "Error at epoch :  3000  =  2.128722667694092\n",
            "Error at epoch :  3100  =  1.9839496612548828\n",
            "Error at epoch :  3200  =  2.4595980644226074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhqmMUxirNde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jVQThCdrNdi",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of the representation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oEOO0xkTrNdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_pvdm = None\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"/dev/shm/tensorflow_models/model_pvdm_batch_training.ckpt\")\n",
        "    print(\"Model restored.\")\n",
        "    doc2vec = doc_pvdm = doc_embedding.eval()\n",
        "    #performanceTest(doc2vec, list(imdb_data['sentiment']), method=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPEA81VtrNdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_pvdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmr1COC8rNdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "model = TSNE(perplexity=50, n_iter=5000)\n",
        "Z = model.fit_transform(doc_pvdm) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DxTAp8PrNds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['axis1'] = Z[:, 0]\n",
        "df['axis2'] = Z[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHlA5WQorNdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aCIqolHrNdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkz7APSUrNd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ggplot import *\n",
        "myplot = ggplot(aes(x='axis1', y='axis2', color='label'), data=df)  + geom_point()\n",
        "myplot.save('myplot.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk5EIOvqrNd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}