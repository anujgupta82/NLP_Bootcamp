{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from load_imdb_data import load_imdb_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"../../data/intent_data.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "labels = []\n",
    "\n",
    "for item in data:\n",
    "    temp = item.split('   ')\n",
    "    sentence.append(temp[0].strip())\n",
    "    labels.append(temp[1].strip().replace(\"\\n\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = [line.decode('utf-8').strip() for line in sentence]\n",
    "sentence = [line.strip() for line in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['review'] = sentence\n",
    "df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'].isin(['food', 'travel', 'career'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really want a hot dog . My co-workers went t...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanna go to the beach , but ... Where is the...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really look forward to Egypt next week ! Its...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I should buy more oatmeal cookies .. I just at...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want pancakes , should I make some ???</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   label\n",
       "0  I really want a hot dog . My co-workers went t...    food\n",
       "1  I wanna go to the beach , but ... Where is the...  travel\n",
       "2  I really look forward to Egypt next week ! Its...  travel\n",
       "3  I should buy more oatmeal cookies .. I just at...    food\n",
       "4           I want pancakes , should I make some ???    food"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDocs(documents, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    This functions takes in a collection of documents and generates a vocabulary based on the size given in input. \n",
    "    It returns a representation for each document in the list of input documents. \n",
    "    \"\"\"\n",
    "    vocab = {} \n",
    "    doc_id = 0 \n",
    "    doc_ids = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_ids.append(doc_id)                          # Give an ID to each document \n",
    "        doc_id += 1\n",
    "        \n",
    "        for word in nltk.word_tokenize(doc):            # Generate a vocabulary while iterating threw the documents \n",
    "            if word not in vocab:\n",
    "                vocab[word] = 1 \n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "    \n",
    "    # Extract the most frequent words based on the vocabulary size \n",
    "    freq_words_list = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    freq_words_set = set([item[0] for item in freq_words_list])\n",
    "    \n",
    "    # Give an index to each word in vocabulary \n",
    "    word2idx = {}         \n",
    "    index_word = 0\n",
    "    for word in freq_words_set:\n",
    "        word2idx[word] = index_word\n",
    "        index_word += 1\n",
    "    word2idx['UNK'] = index_word\n",
    "    \n",
    "    doc_repr = []                          # Represent each document with representation based on the vocabulary  \n",
    "    for doc in documents:\n",
    "        temp = []\n",
    "        for w in doc:\n",
    "            if w in word2idx:\n",
    "                temp.append(word2idx[w])\n",
    "            else:\n",
    "                temp.append(word2idx['UNK'])\n",
    "        doc_repr.append(temp)\n",
    "        \n",
    "    return documents, doc_ids, word2idx, doc_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs, doc_ids, word2ids, doc_repr = processDocs(df['review'], vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 591 1901 591\n"
     ]
    }
   ],
   "source": [
    "print(len(docs), len(doc_ids), len(word2ids), len(doc_repr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture - PV-DBOW Distributed Bag of Words version of Paragraph Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding batchsizes for speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_list = []\n",
    "\n",
    "def generate_batch_pvdbow(doc_ids, doc_repr, sample_size=20, batch_size=1000, window_size=7):\n",
    "    global bucket_list\n",
    "\n",
    "    docs_ids_to_select = list(set(doc_ids) - set(bucket_list))\n",
    "    \n",
    "    \n",
    "    if len(docs_ids_to_select) < batch_size//sample_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = doc_ids\n",
    "        \n",
    "    index = 0 \n",
    "    train_dX = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, window_size), dtype=np.int32)\n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size//sample_size)    # Choose set of random documents \n",
    "\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    for id_ in random_docs:\n",
    "        for j in range(sample_size):                                 # Generating a dataset of sample size \n",
    "            random_index = random.randint(0, len(doc_repr[id_]) - window_size)\n",
    "            sample_window = doc_repr[id_][random_index: random_index + window_size]\n",
    "            train_dX[index] = id_\n",
    "            train_label[index] = sample_window\n",
    "            index += 1\n",
    "    return train_dX, train_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_size = len(docs)\n",
    "embedding_size_d = 100\n",
    "embedding_size_w = 100\n",
    "vocab_size = len(word2ids)\n",
    "window_size = 7\n",
    "n_neg_samples = 20\n",
    "learning_rate = 10e-2\n",
    "epochs = 10001\n",
    "batch_size=1000\n",
    "mu=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders for training \n",
    "train_dX = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "train_label = tf.placeholder(tf.int32, shape=[batch_size, window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding_np = np.random.randn(doc_size, embedding_size_d)/np.sqrt(doc_size + embedding_size_d)\n",
    "word_embedding_np = np.random.randn(vocab_size, embedding_size_w)/np.sqrt(vocab_size + embedding_size_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrix for doc_embedding and word_embedding \n",
    "doc_embedding = tf.Variable(doc_embedding_np.astype(np.float32), name=\"doc_embedding\")\n",
    "word_embedding = tf.Variable(word_embedding_np.astype(np.float32),name=\"word_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for the output unit \n",
    "weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size_d], \n",
    "                                       stddev=1.0 / math.sqrt(vocab_size)))\n",
    "biases = tf.Variable(tf.zeros(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1901, 100)\n"
     ]
    }
   ],
   "source": [
    "print (weights.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the doc2vec from the doc_embedding \n",
    "embed = tf.nn.embedding_lookup(doc_embedding, train_dX[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sampled_softmax_loss(weights=weights, \\\n",
    "                                  biases=biases, \\\n",
    "                                  labels=train_label, \\\n",
    "                                  inputs=embed, \\\n",
    "                                  num_sampled=n_neg_samples, \\\n",
    "                                  num_classes=vocab_size, \\\n",
    "                                  num_true=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"mkdir /dev/shm/tensorflow_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch :  0  =  2.773125648498535\n",
      "Error at epoch :  1000  =  2.086777448654175\n",
      "Error at epoch :  2000  =  2.0167131423950195\n",
      "Error at epoch :  3000  =  1.9949673414230347\n",
      "Error at epoch :  4000  =  2.0192675590515137\n",
      "Error at epoch :  5000  =  2.0004444122314453\n",
      "Error at epoch :  6000  =  2.041379690170288\n",
      "Error at epoch :  7000  =  1.977998971939087\n",
      "Error at epoch :  8000  =  1.997300386428833\n",
      "Error at epoch :  9000  =  2.0414466857910156\n",
      "Error at epoch :  10000  =  1.9742205142974854\n",
      "Model saved in file: /dev/shm/tensorflow_models/model_pvdbow_batch_training.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(epochs):\n",
    "        epoch_error = 0.0\n",
    "        temp_dX, temp_labels = generate_batch_pvdbow(doc_ids=doc_ids, doc_repr=doc_repr)\n",
    "        feed_dict = {train_dX : temp_dX,train_label : temp_labels}\n",
    "        op, l = sess.run([optimizer, loss], \n",
    "                                    feed_dict=feed_dict)\n",
    "        \n",
    "        epoch_error += l\n",
    "                \n",
    "        if step % 1000 == 0:\n",
    "            print (\"Error at epoch : \", step, \" = \", epoch_error)\n",
    "            \n",
    "    save_path = saver.save(sess, \"/dev/shm/tensorflow_models/model_pvdbow_batch_training.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /dev/shm/tensorflow_models/model_pvdbow_batch_training.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "doc_pvdbow = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"/dev/shm/tensorflow_models/model_pvdbow_batch_training.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    doc2vec = doc_pvdbow = doc_embedding.eval()\n",
    "    #performanceTest(doc2vec, list(imdb_data['sentiment']), method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(perplexity=50, n_iter=5000)\n",
    "Z = model.fit_transform(doc_pvdbow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['axis1'] = Z[:, 0]\n",
    "df['axis2'] = Z[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>axis1</th>\n",
       "      <th>axis2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really want a hot dog . My co-workers went t...</td>\n",
       "      <td>food</td>\n",
       "      <td>-0.686677</td>\n",
       "      <td>-1.011223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanna go to the beach , but ... Where is the...</td>\n",
       "      <td>travel</td>\n",
       "      <td>0.963187</td>\n",
       "      <td>-1.709334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really look forward to Egypt next week ! Its...</td>\n",
       "      <td>travel</td>\n",
       "      <td>1.409287</td>\n",
       "      <td>-1.347921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I should buy more oatmeal cookies .. I just at...</td>\n",
       "      <td>food</td>\n",
       "      <td>-1.954101</td>\n",
       "      <td>-0.941361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want pancakes , should I make some ???</td>\n",
       "      <td>food</td>\n",
       "      <td>0.866388</td>\n",
       "      <td>1.510582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review   label     axis1  \\\n",
       "0  I really want a hot dog . My co-workers went t...    food -0.686677   \n",
       "1  I wanna go to the beach , but ... Where is the...  travel  0.963187   \n",
       "2  I really look forward to Egypt next week ! Its...  travel  1.409287   \n",
       "3  I should buy more oatmeal cookies .. I just at...    food -1.954101   \n",
       "4           I want pancakes , should I make some ???    food  0.866388   \n",
       "\n",
       "      axis2  \n",
       "0 -1.011223  \n",
       "1 -1.709334  \n",
       "2 -1.347921  \n",
       "3 -0.941361  \n",
       "4  1.510582  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/home/piyush/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b0f182f6ff43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mggplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mggplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'axis1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'axis2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mgeom_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeom_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_blank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_boxplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_jitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_histogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_density\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_hline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_vline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_abline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_tile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_rect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_bin2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_ribbon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_now_its_art\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_violin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_errorbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_polygon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstat_smooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_density\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfacets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfacet_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacet_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFacet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstat_density\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstat_density\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstat_smooth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstat_smooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/stats/stat_smooth.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmoothers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/ggplot/stats/smoothers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                         unicode_literals)\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Timestamp'"
     ]
    }
   ],
   "source": [
    "from ggplot import *\n",
    "ggplot(aes(x='axis1', y='axis2', color='label'), data=df)  + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
