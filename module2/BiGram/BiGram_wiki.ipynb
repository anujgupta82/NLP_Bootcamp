{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "BiGram_wiki.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAtotZPa90JS",
        "colab_type": "text"
      },
      "source": [
        "---------\n",
        "\n",
        "# BiGram Model\n",
        "\n",
        "----------\n",
        "\n",
        "#### Built on lines of  &nbsp;&nbsp;&nbsp;&nbsp;   [representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb](http://localhost:8888/notebooks/representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb)\n",
        "\n",
        "### Added \n",
        "    1) tensorboard network visualizations   \n",
        "    2) tensorboard loss visualizations\n",
        "    3) similar words to words in validation data\n",
        "\n",
        "#### Author : Anuj\n",
        "\n",
        "#### Uses Wikipedia Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsYWctUf90JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvUPruVG90Jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngpPRTmn-d8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbNimudI90Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append(\"/content/drive//GoogleDrive_Utils/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r85qoEfr90Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from readWikiData import get_wikipedia_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cvpAp1yz90Jv",
        "colab_type": "text"
      },
      "source": [
        "## Load the data file - map tokens to Ids, convert data to Ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9kVykNT90Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wiki_data(n_vocab_=1000):\n",
        "    sentences, word2idx,  _, _ = get_wikipedia_data(n_vocab=n_vocab_, n_files=10, by_paragraph=True)\n",
        "    training_data = []\n",
        "    vocab_size = len(word2idx)\n",
        "    for sentence in sentences:\n",
        "        for elem1, elem2 in zip(sentence[:-1], sentence[1:]):\n",
        "            training_data.append((elem1, elem2))\n",
        "    \n",
        "    # this destroys the order of words in a wondow but for bigram its harmless\n",
        "    # all we want is - pair of all bigrams\n",
        "    training_data = list(set(training_data))   \n",
        "    \n",
        "    idx2word = {v:k for k, v in word2idx.items()}\n",
        "    return len(word2idx), training_data, word2idx, idx2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XgSpI090Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size, training_data, word2idx, idx2word = get_wiki_data(n_vocab_=9999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc_SvTW490J8",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed27e0ba-0d47-4f78-cfdc-3655e85c3eec"
      },
      "source": [
        "print (vocab_size)\n",
        "print (type(training_data))\n",
        "print (len(training_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "<class 'list'>\n",
            "1664638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXPQjfMf90KC",
        "colab_type": "code",
        "colab": {},
        "outputId": "cf410f90-5103-4d18-826b-81aa5512007b"
      },
      "source": [
        "print (len(word2idx.keys()))\n",
        "print (len(idx2word.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plS97TRL90KK",
        "colab_type": "text"
      },
      "source": [
        "## Build validation set - randomly choose 100 keys from idx2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO9-U2tv90KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# randomly pick some validation words from data\n",
        "\n",
        "validation_size = 32\n",
        "#validation_set = random.sample(idx2word.keys(), validation_size)\n",
        "validation_set = random.sample(idx2word.keys(), validation_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_aUNBJV90KP",
        "colab_type": "code",
        "colab": {},
        "outputId": "de476163-b0f8-4fe7-8db7-b0fda877b0fd"
      },
      "source": [
        "print (validation_set)\n",
        "print ([idx2word[index] for index in validation_set])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9372, 9892, 4577, 3605, 8304, 4949, 2058, 8160, 7753, 8801, 5758, 6495, 2091, 9691, 7360, 2346, 1076, 6165, 2109, 6075, 2721, 736, 3125, 6011, 8746, 9144, 1177, 835, 9916, 7342, 5605, 6128]\n",
            "['cerebral', '1700', 'algae', 'rear', '84', 'genres', '1972', 'deny', 'dive', 'deliberate', 'restaurant', 'encyclopedia', '1960', 'benedictine', 'viable', 'currency', 'necessary', 'linking', 'manner', 'farms', 'bulgaria', '100', 'cuisine', 'smiths', 'pursuing', 'asylum', 'marriage', 'mainly', 'suffrage', 'therapeutic', 'norm', 'clusters']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb-KQdCv90KU",
        "colab_type": "text"
      },
      "source": [
        "### Get batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vplQMUkR90KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# contains list of pairs that have already been selected\n",
        "bucket_list = []\n",
        "\n",
        "def getNextBatch(bi_grams_, batch_size=1000):\n",
        "    \n",
        "    global bucket_list\n",
        "    \n",
        "    # list of possible pairs to pick from\n",
        "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
        "    \n",
        "    # once you exhaust the possible pais, reset\n",
        "    if len(docs_ids_to_select) < batch_size:\n",
        "        bucket_list = []\n",
        "        docs_ids_to_select = bi_grams_\n",
        "        \n",
        "    # Initialize two variables \n",
        "    train_X = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    \n",
        "    # pick a random chunks of pairs \n",
        "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
        "    bucket_list += random_docs\n",
        "    \n",
        "    index = 0 \n",
        "    \n",
        "    # Iterate threw all the docs \n",
        "    for item in random_docs:\n",
        "        train_X[index] = item[0]\n",
        "        train_label[index] = item[1]  \n",
        "        index += 1\n",
        "        \n",
        "    #flatten list of lists to a single list\n",
        "    train_X = list(itertools.chain(*train_X))\n",
        "    train_label = list(itertools.chain(*train_label))\n",
        "            \n",
        "    return train_X, train_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFhnCfhp90Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X, Y = getNextBatch(bi_grams_=training_data, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sDeCffq90Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print len(X), len(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-Z5Y3A90Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print X[:10], Y[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvHOB-fl90Kv",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RqUxRng90Kw",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1068755-37af-4246-e249-824f41bd454b"
      },
      "source": [
        "batch_size=32\n",
        "num_batches = len(training_data)/batch_size\n",
        "\n",
        "print (\"Number of batches = %d\" %num_batches)\n",
        "\n",
        "\n",
        "embedding_dims = 128"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches = 52019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngHGuEci90K0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='X_var')\n",
        "Y = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='Y_var')\n",
        "valid_X = tf.Variable(validation_set, dtype=tf.int32, name='X_valid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFT_ucSX90K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_oh = tf.one_hot(indices=X, depth=vocab_size, name='Converting_Y_to_Y_oh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-It5sGN90K_",
        "colab_type": "code",
        "colab": {},
        "outputId": "561607fb-0fa7-45a9-b1af-f424d8a2ba40"
      },
      "source": [
        "print (X.get_shape())\n",
        "print (Y.get_shape())\n",
        "print (y_oh.get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32,)\n",
            "(32,)\n",
            "(32, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnt-0PIm90LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer_1 = tf.Variable(tf.truncated_normal(\n",
        "    shape=(vocab_size, embedding_dims),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Embeddings_Matrix\") \n",
        "embeded = tf.nn.embedding_lookup(embedding_layer_1, ids=X, name=\"Embedding_LookUp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8kes6SM90LW",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1e3fc7d-b324-4a26-d91f-47a9fba7c561"
      },
      "source": [
        "embeded.get_shape()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(32), Dimension(128)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojr1d9v190Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#softmax weights, bias\n",
        "W = tf.Variable(tf.truncated_normal(\n",
        "    shape=(embedding_dims, vocab_size),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Softmax_Weights_Matrix\")\n",
        "b = tf.Variable(tf.zeros(shape=(vocab_size,)), name=\"Softmax_Bias_Vector\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnRPvwlE90Lr",
        "colab_type": "code",
        "colab": {},
        "outputId": "ebcc3d1a-5835-4a18-e70a-28ca1bcc81d9"
      },
      "source": [
        "logits = tf.add(tf.matmul(embeded, W, name=\"WX\"), b, name=\"WX_plus_b\")\n",
        "\n",
        "#logits = tf.add(tf.matmul(embed, softmax_weights, name=\"WX\"), softmax_bias, name=\"WX_plus_b\")\n",
        "\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_oh, name=\"Compute_Loss\")\n",
        "#mean_loss = tf.reduce_mean(loss)\n",
        "mean_loss = tf.reduce_mean(loss, name=\"Compute_mean_loss\")\n",
        "\n",
        "tf.summary.scalar(\"mean_loss\", mean_loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bap5LLb990Lu",
        "colab_type": "code",
        "colab": {},
        "outputId": "6bd31ea4-74b3-40dc-9a71-03bb5f02a2fe"
      },
      "source": [
        "print (logits.get_shape())\n",
        "print (y_oh.get_shape())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10000)\n",
            "(32, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P0DeTk_90Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K33YWHHh90L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(0.5, name=\"Optimizer\").minimize(mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA3zI9LL90L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZICNTCz90MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNiMtq_j90MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute L2 norm for cosine similarity\n",
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer_1), axis=1, keepdims=True))\n",
        "normalised_embeddings = embedding_layer_1 / norm\n",
        "\n",
        "# get validation set embeddings\n",
        "validation_data_embeddings = tf.nn.embedding_lookup(normalised_embeddings, ids=valid_X, name=\"validation_embeddings_lookup\")\n",
        "\n",
        "# similarity score of validation embeddings w.r.t normalised= dot product between validation_data_embeddings and mornalised embeddings\n",
        "similarity = tf.matmul(validation_data_embeddings, tf.transpose(normalised_embeddings))  # C.A = C x transpose(A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiZrJ1yS90MI",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vz7TwCd90MJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5300fa6-063b-4237-bb37-b746936d236d"
      },
      "source": [
        "num_of_epochs = 20\n",
        "LOG_DIR = './bigram_wiki_chk_pts'\n",
        "\n",
        "print (\"Number of batches = %d\" %num_batches)\n",
        "print (\"Number of epochs = %d\" %num_of_epochs)\n",
        "\n",
        "\n",
        "validation_size = 8"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches = 52019\n",
            "Number of epochs = 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O46VJEp790MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# demo params \n",
        "\n",
        "num_of_epochs = 5\n",
        "num_batches = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "mIre6tXs90MS",
        "colab_type": "code",
        "colab": {},
        "outputId": "f0127bc1-dfba-470f-d0af-a208d690a62e"
      },
      "source": [
        "\n",
        "\n",
        "# A SIMPLE saver() to save the model\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    # writer to write graph to tensorboard\n",
        "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
        "\n",
        "    tf.global_variables_initializer().run()\n",
        "    print (\"initialised\\n\")\n",
        "\n",
        "    for epoch_id in range(num_of_epochs):\n",
        "\n",
        "        av_batch_loss = 0\n",
        "\n",
        "        for batch_id in range(num_batches):\n",
        "\n",
        "            X_, Y_ = getNextBatch(bi_grams_=training_data, batch_size=batch_size)\n",
        "\n",
        "            feed_dict = {}\n",
        "            feed_dict[X] = X_\n",
        "            feed_dict[Y] = Y_\n",
        "\n",
        "            batch_loss, _, summary = sess.run([mean_loss, optimizer, summary_op], feed_dict=feed_dict)\n",
        "            \n",
        "            #writer.add_summary(batch_loss, epoch) \n",
        "            step_id = epoch_id * num_batches + batch_id\n",
        "            print (\"step_id = %d\" %step_id)\n",
        "            writer.add_summary(summary, global_step=step_id)\n",
        "\n",
        "            av_batch_loss += batch_loss\n",
        "            \n",
        "            if batch_id % 500 == 0:\n",
        "                print (\"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss))\n",
        "            \n",
        "            if batch_id % 1000 == 0:\n",
        "                print (\"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss))\n",
        "                \n",
        "                #print validation data\n",
        "                sim = similarity.eval() # compute similarity\n",
        "                \n",
        "                #iterate over each validation example\n",
        "                \n",
        "                for i in range(validation_size):\n",
        "                    word = idx2word[validation_set[i]]\n",
        "                    top_k = 8\n",
        "                    # sort indexes and pick top k. we take 1:top_k+1 since 0th top pick will the same word itself\n",
        "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
        "                    \n",
        "                    #nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
        "                    \n",
        "                    log = '\\t Nearest to %s : ' %word\n",
        "                    for k in range(top_k):\n",
        "                        nearest_word = idx2word[nearest[k]]\n",
        "                        log = '%s %s,' %(log, nearest_word)\n",
        "                    print (log)        \n",
        "\n",
        "        print (\"\\nFor epoch = %d, Av loss = %f\" %(epoch_id, av_batch_loss/num_batches))\n",
        "        \n",
        "        #batch.reset()\n",
        "        \n",
        "    save_path = saver.save(sess, LOG_DIR)\n",
        "    print(\"Model saved in file: %s\" % save_path)\n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialised\n",
            "\n",
            "step_id = 0\n",
            "\n",
            "For epoch = 0, batch id = 0, batch loss = 32.715607\n",
            "\n",
            "\n",
            "For epoch = 0, batch id = 0, batch loss = 32.715607\n",
            "\n",
            "\t Nearest to cerebral :  transferred, mining, proprietary, seasons, 1000, toll, 1900, four,\n",
            "\t Nearest to 1700 :  car, fate, verse, junction, meaning, includes, apartment, end,\n",
            "\t Nearest to algae :  careful, therapies, ensure, such, split, persistent, age, sony,\n",
            "\t Nearest to rear :  rom, defenders, commodity, conscription, lawyers, drake, skin, chemicals,\n",
            "\t Nearest to 84 :  battleship, hitchcock, 1846, czechoslovakia, fleet, recovery, achieving, transformation,\n",
            "\t Nearest to genres :  disappeared, toronto, proclamation, circulated, absent, 2000s, implementations, ethanol,\n",
            "\t Nearest to 1972 :  commonwealth, dog, epistle, rebuilding, happened, keeping, registration, peter,\n",
            "\t Nearest to deny :  wild, knife, active, 62, warner, pursuit, sends, commodities,\n",
            "step_id = 1\n",
            "step_id = 2\n",
            "step_id = 3\n",
            "step_id = 4\n",
            "step_id = 5\n",
            "step_id = 6\n",
            "step_id = 7\n",
            "step_id = 8\n",
            "step_id = 9\n",
            "step_id = 10\n",
            "step_id = 11\n",
            "step_id = 12\n",
            "step_id = 13\n",
            "step_id = 14\n",
            "step_id = 15\n",
            "step_id = 16\n",
            "step_id = 17\n",
            "step_id = 18\n",
            "step_id = 19\n",
            "step_id = 20\n",
            "step_id = 21\n",
            "step_id = 22\n",
            "step_id = 23\n",
            "step_id = 24\n",
            "step_id = 25\n",
            "step_id = 26\n",
            "step_id = 27\n",
            "step_id = 28\n",
            "step_id = 29\n",
            "step_id = 30\n",
            "step_id = 31\n",
            "step_id = 32\n",
            "step_id = 33\n",
            "step_id = 34\n",
            "step_id = 35\n",
            "step_id = 36\n",
            "step_id = 37\n",
            "step_id = 38\n",
            "step_id = 39\n",
            "step_id = 40\n",
            "step_id = 41\n",
            "step_id = 42\n",
            "step_id = 43\n",
            "step_id = 44\n",
            "step_id = 45\n",
            "step_id = 46\n",
            "step_id = 47\n",
            "step_id = 48\n",
            "step_id = 49\n",
            "\n",
            "For epoch = 0, Av loss = 31.934274\n",
            "step_id = 50\n",
            "\n",
            "For epoch = 1, batch id = 0, batch loss = 30.155050\n",
            "\n",
            "\n",
            "For epoch = 1, batch id = 0, batch loss = 30.155050\n",
            "\n",
            "\t Nearest to cerebral :  transferred, mining, proprietary, seasons, 1000, toll, 1900, four,\n",
            "\t Nearest to 1700 :  car, fate, verse, junction, includes, meaning, apartment, youth,\n",
            "\t Nearest to algae :  careful, therapies, ensure, such, split, persistent, age, sony,\n",
            "\t Nearest to rear :  rom, defenders, commodity, conscription, lawyers, drake, skin, chemicals,\n",
            "\t Nearest to 84 :  battleship, hitchcock, 1846, czechoslovakia, fleet, recovery, achieving, transformation,\n",
            "\t Nearest to genres :  disappeared, toronto, proclamation, circulated, absent, 2000s, implementations, ethanol,\n",
            "\t Nearest to 1972 :  commonwealth, dog, epistle, rebuilding, happened, keeping, registration, peter,\n",
            "\t Nearest to deny :  wild, knife, active, warner, 62, pursuit, sends, commodities,\n",
            "step_id = 51\n",
            "step_id = 52\n",
            "step_id = 53\n",
            "step_id = 54\n",
            "step_id = 55\n",
            "step_id = 56\n",
            "step_id = 57\n",
            "step_id = 58\n",
            "step_id = 59\n",
            "step_id = 60\n",
            "step_id = 61\n",
            "step_id = 62\n",
            "step_id = 63\n",
            "step_id = 64\n",
            "step_id = 65\n",
            "step_id = 66\n",
            "step_id = 67\n",
            "step_id = 68\n",
            "step_id = 69\n",
            "step_id = 70\n",
            "step_id = 71\n",
            "step_id = 72\n",
            "step_id = 73\n",
            "step_id = 74\n",
            "step_id = 75\n",
            "step_id = 76\n",
            "step_id = 77\n",
            "step_id = 78\n",
            "step_id = 79\n",
            "step_id = 80\n",
            "step_id = 81\n",
            "step_id = 82\n",
            "step_id = 83\n",
            "step_id = 84\n",
            "step_id = 85\n",
            "step_id = 86\n",
            "step_id = 87\n",
            "step_id = 88\n",
            "step_id = 89\n",
            "step_id = 90\n",
            "step_id = 91\n",
            "step_id = 92\n",
            "step_id = 93\n",
            "step_id = 94\n",
            "step_id = 95\n",
            "step_id = 96\n",
            "step_id = 97\n",
            "step_id = 98\n",
            "step_id = 99\n",
            "\n",
            "For epoch = 1, Av loss = 28.988072\n",
            "step_id = 100\n",
            "\n",
            "For epoch = 2, batch id = 0, batch loss = 26.737217\n",
            "\n",
            "\n",
            "For epoch = 2, batch id = 0, batch loss = 26.737217\n",
            "\n",
            "\t Nearest to cerebral :  transferred, mining, proprietary, seasons, 1000, toll, 1900, four,\n",
            "\t Nearest to 1700 :  car, fate, verse, junction, meaning, includes, apartment, youth,\n",
            "\t Nearest to algae :  careful, therapies, ensure, such, split, persistent, age, sony,\n",
            "\t Nearest to rear :  rom, defenders, commodity, conscription, lawyers, drake, skin, chemicals,\n",
            "\t Nearest to 84 :  battleship, hitchcock, 1846, czechoslovakia, fleet, recovery, achieving, transformation,\n",
            "\t Nearest to genres :  disappeared, toronto, proclamation, circulated, absent, 2000s, implementations, ethanol,\n",
            "\t Nearest to 1972 :  commonwealth, dog, epistle, rebuilding, happened, keeping, registration, peter,\n",
            "\t Nearest to deny :  wild, knife, active, warner, 62, pursuit, sends, commodities,\n",
            "step_id = 101\n",
            "step_id = 102\n",
            "step_id = 103\n",
            "step_id = 104\n",
            "step_id = 105\n",
            "step_id = 106\n",
            "step_id = 107\n",
            "step_id = 108\n",
            "step_id = 109\n",
            "step_id = 110\n",
            "step_id = 111\n",
            "step_id = 112\n",
            "step_id = 113\n",
            "step_id = 114\n",
            "step_id = 115\n",
            "step_id = 116\n",
            "step_id = 117\n",
            "step_id = 118\n",
            "step_id = 119\n",
            "step_id = 120\n",
            "step_id = 121\n",
            "step_id = 122\n",
            "step_id = 123\n",
            "step_id = 124\n",
            "step_id = 125\n",
            "step_id = 126\n",
            "step_id = 127\n",
            "step_id = 128\n",
            "step_id = 129\n",
            "step_id = 130\n",
            "step_id = 131\n",
            "step_id = 132\n",
            "step_id = 133\n",
            "step_id = 134\n",
            "step_id = 135\n",
            "step_id = 136\n",
            "step_id = 137\n",
            "step_id = 138\n",
            "step_id = 139\n",
            "step_id = 140\n",
            "step_id = 141\n",
            "step_id = 142\n",
            "step_id = 143\n",
            "step_id = 144\n",
            "step_id = 145\n",
            "step_id = 146\n",
            "step_id = 147\n",
            "step_id = 148\n",
            "step_id = 149\n",
            "\n",
            "For epoch = 2, Av loss = 27.729566\n",
            "step_id = 150\n",
            "\n",
            "For epoch = 3, batch id = 0, batch loss = 26.217876\n",
            "\n",
            "\n",
            "For epoch = 3, batch id = 0, batch loss = 26.217876\n",
            "\n",
            "\t Nearest to cerebral :  transferred, mining, proprietary, seasons, 1000, toll, 1900, drinking,\n",
            "\t Nearest to 1700 :  car, fate, verse, junction, meaning, includes, youth, apartment,\n",
            "\t Nearest to algae :  careful, therapies, ensure, such, split, persistent, age, sony,\n",
            "\t Nearest to rear :  rom, defenders, commodity, conscription, lawyers, drake, skin, chemicals,\n",
            "\t Nearest to 84 :  battleship, hitchcock, 1846, czechoslovakia, fleet, recovery, achieving, transformation,\n",
            "\t Nearest to genres :  disappeared, toronto, proclamation, circulated, absent, 2000s, implementations, ethanol,\n",
            "\t Nearest to 1972 :  commonwealth, dog, epistle, rebuilding, keeping, happened, registration, peter,\n",
            "\t Nearest to deny :  wild, knife, active, warner, 62, pursuit, sends, commodities,\n",
            "step_id = 151\n",
            "step_id = 152\n",
            "step_id = 153\n",
            "step_id = 154\n",
            "step_id = 155\n",
            "step_id = 156\n",
            "step_id = 157\n",
            "step_id = 158\n",
            "step_id = 159\n",
            "step_id = 160\n",
            "step_id = 161\n",
            "step_id = 162\n",
            "step_id = 163\n",
            "step_id = 164\n",
            "step_id = 165\n",
            "step_id = 166\n",
            "step_id = 167\n",
            "step_id = 168\n",
            "step_id = 169\n",
            "step_id = 170\n",
            "step_id = 171\n",
            "step_id = 172\n",
            "step_id = 173\n",
            "step_id = 174\n",
            "step_id = 175\n",
            "step_id = 176\n",
            "step_id = 177\n",
            "step_id = 178\n",
            "step_id = 179\n",
            "step_id = 180\n",
            "step_id = 181\n",
            "step_id = 182\n",
            "step_id = 183\n",
            "step_id = 184\n",
            "step_id = 185\n",
            "step_id = 186\n",
            "step_id = 187\n",
            "step_id = 188\n",
            "step_id = 189\n",
            "step_id = 190\n",
            "step_id = 191\n",
            "step_id = 192\n",
            "step_id = 193\n",
            "step_id = 194\n",
            "step_id = 195\n",
            "step_id = 196\n",
            "step_id = 197\n",
            "step_id = 198\n",
            "step_id = 199\n",
            "\n",
            "For epoch = 3, Av loss = 25.878087\n",
            "step_id = 200\n",
            "\n",
            "For epoch = 4, batch id = 0, batch loss = 28.653530\n",
            "\n",
            "\n",
            "For epoch = 4, batch id = 0, batch loss = 28.653530\n",
            "\n",
            "\t Nearest to cerebral :  transferred, mining, proprietary, seasons, 1000, toll, 1900, drinking,\n",
            "\t Nearest to 1700 :  car, fate, verse, junction, meaning, includes, youth, apartment,\n",
            "\t Nearest to algae :  careful, therapies, such, ensure, split, persistent, age, sony,\n",
            "\t Nearest to rear :  rom, defenders, commodity, conscription, lawyers, drake, skin, chemicals,\n",
            "\t Nearest to 84 :  battleship, hitchcock, 1846, czechoslovakia, fleet, recovery, transformation, participant,\n",
            "\t Nearest to genres :  disappeared, toronto, proclamation, circulated, absent, 2000s, implementations, ethanol,\n",
            "\t Nearest to 1972 :  commonwealth, dog, epistle, rebuilding, keeping, happened, registration, peter,\n",
            "\t Nearest to deny :  wild, knife, active, warner, 62, pursuit, sends, commodities,\n",
            "step_id = 201\n",
            "step_id = 202\n",
            "step_id = 203\n",
            "step_id = 204\n",
            "step_id = 205\n",
            "step_id = 206\n",
            "step_id = 207\n",
            "step_id = 208\n",
            "step_id = 209\n",
            "step_id = 210\n",
            "step_id = 211\n",
            "step_id = 212\n",
            "step_id = 213\n",
            "step_id = 214\n",
            "step_id = 215\n",
            "step_id = 216\n",
            "step_id = 217\n",
            "step_id = 218\n",
            "step_id = 219\n",
            "step_id = 220\n",
            "step_id = 221\n",
            "step_id = 222\n",
            "step_id = 223\n",
            "step_id = 224\n",
            "step_id = 225\n",
            "step_id = 226\n",
            "step_id = 227\n",
            "step_id = 228\n",
            "step_id = 229\n",
            "step_id = 230\n",
            "step_id = 231\n",
            "step_id = 232\n",
            "step_id = 233\n",
            "step_id = 234\n",
            "step_id = 235\n",
            "step_id = 236\n",
            "step_id = 237\n",
            "step_id = 238\n",
            "step_id = 239\n",
            "step_id = 240\n",
            "step_id = 241\n",
            "step_id = 242\n",
            "step_id = 243\n",
            "step_id = 244\n",
            "step_id = 245\n",
            "step_id = 246\n",
            "step_id = 247\n",
            "step_id = 248\n",
            "step_id = 249\n",
            "\n",
            "For epoch = 4, Av loss = 24.137654\n",
            "Model saved in file: ./bigram_wiki_chk_pts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCnqb61b90MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cBWQcm-g90MZ",
        "colab_type": "text"
      },
      "source": [
        "# Plot the Embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWoWxYIC90Ma",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z_9GMmOd90Mb",
        "colab_type": "code",
        "colab": {},
        "outputId": "c3791957-636d-4390-d91a-047d69600ae0"
      },
      "source": [
        "num_of_epochs = 5\n",
        "num_batches = 50\n",
        "\n",
        "for epoch_id in range(num_of_epochs):\n",
        "    for batch_id in range(num_batches):\n",
        "        \n",
        "        step_id = epoch_id * num_batches + batch_id\n",
        "        \n",
        "        print (step_id)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwhIYkEm90Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}