{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# BiGram Model\n",
    "\n",
    "----------\n",
    "\n",
    "#### Built on lines of  &nbsp;&nbsp;&nbsp;&nbsp;   [representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb](http://localhost:8888/notebooks/representation_learning/word2vec/BiGram_barebones_1_wiki.ipynb)\n",
    "\n",
    "### Added \n",
    "    1) tensorboard network visualizations   \n",
    "    2) tensorboard loss visualizations\n",
    "    3) similar words to words in validation data\n",
    "\n",
    "#### Author : Anuj\n",
    "\n",
    "#### Uses Wikipedia Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../Utils/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readWikiData import get_wikipedia_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the data file - map tokens to Ids, convert data to Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_data(n_vocab_=1000):\n",
    "    sentences, word2idx,  _, _ = get_wikipedia_data(n_vocab=n_vocab_, n_files=10, by_paragraph=True)\n",
    "    training_data = []\n",
    "    vocab_size = len(word2idx)\n",
    "    for sentence in sentences:\n",
    "        for elem1, elem2 in zip(sentence[:-1], sentence[1:]):\n",
    "            training_data.append((elem1, elem2))\n",
    "    \n",
    "    # this destroys the order of words in a wondow but for bigram its harmless\n",
    "    # all we want is - pair of all bigrams\n",
    "    training_data = list(set(training_data))   \n",
    "    \n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return len(word2idx), training_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, training_data, word2idx, idx2word = get_wiki_data(n_vocab_=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'list'>\n",
      "1370121\n"
     ]
    }
   ],
   "source": [
    "print (vocab_size)\n",
    "print (type(training_data))\n",
    "print (len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(word2idx.keys()))\n",
    "print (len(idx2word.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build validation set - randomly choose 100 keys from idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick some validation words from data\n",
    "\n",
    "validation_size = 32\n",
    "#validation_set = random.sample(idx2word.keys(), validation_size)\n",
    "validation_set = random.sample(idx2word.keys(), validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8479, 2768, 734, 6604, 4239, 4657, 4622, 8676, 6007, 3196, 1252, 6347, 5202, 867, 122, 5415, 3283, 6827, 2409, 5084, 7732, 999, 8828, 9906, 2949, 9340, 2802, 5448, 2560, 6684, 4630, 325]\n",
      "['44', 'f.', 'active', 'operations,', 'gone', 'steam', '1975', '1929', 'looks', 'requirements', '7', 'feeding', 'renewed', 'represented', 'british', 'treating', '1997,', 'salvation', '\"in', 'parks', 'whole.', 'turned', 'scotia', '52', 'one.', 'short-term', 'lasted', 'hear', 'committed', 'dennis', 'proportional', 'seen']\n"
     ]
    }
   ],
   "source": [
    "print (validation_set)\n",
    "print ([idx2word[index] for index in validation_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains list of pairs that have already been selected\n",
    "bucket_list = []\n",
    "\n",
    "def getNextBatch(bi_grams_, batch_size=1000):\n",
    "    \n",
    "    global bucket_list\n",
    "    \n",
    "    # list of possible pairs to pick from\n",
    "    docs_ids_to_select = list(set(bi_grams_) - set(bucket_list))\n",
    "    \n",
    "    # once you exhaust the possible pais, reset\n",
    "    if len(docs_ids_to_select) < batch_size:\n",
    "        bucket_list = []\n",
    "        docs_ids_to_select = bi_grams_\n",
    "        \n",
    "    # Initialize two variables \n",
    "    train_X = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    train_label = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # pick a random chunks of pairs \n",
    "    random_docs = random.sample(docs_ids_to_select, batch_size)\n",
    "    bucket_list += random_docs\n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    # Iterate threw all the docs \n",
    "    for item in random_docs:\n",
    "        train_X[index] = item[0]\n",
    "        train_label[index] = item[1]  \n",
    "        index += 1\n",
    "        \n",
    "    #flatten list of lists to a single list\n",
    "    train_X = list(itertools.chain(*train_X))\n",
    "    train_label = list(itertools.chain(*train_label))\n",
    "            \n",
    "    return train_X, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = getNextBatch(bi_grams_=training_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 42816\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "num_batches = len(training_data)/batch_size\n",
    "\n",
    "print (\"Number of batches = %d\" %num_batches)\n",
    "\n",
    "\n",
    "embedding_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='X_var')\n",
    "Y = tf.placeholder(shape=(batch_size,), dtype = tf.int32, name='Y_var')\n",
    "valid_X = tf.Variable(validation_set, dtype=tf.int32, name='X_valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.one_hot(indices=X, depth=vocab_size, name='Converting_Y_to_Y_oh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "(32,)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (X.get_shape())\n",
    "print (Y.get_shape())\n",
    "print (y_oh.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_1 = tf.Variable(tf.truncated_normal(\n",
    "    shape=(vocab_size, embedding_dims),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Embeddings_Matrix\") \n",
    "embeded = tf.nn.embedding_lookup(embedding_layer_1, ids=X, name=\"Embedding_LookUp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(32), Dimension(128)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax weights, bias\n",
    "W = tf.Variable(tf.truncated_normal(\n",
    "    shape=(embedding_dims, vocab_size),mean=0.0, stddev=1.0, dtype=tf.float32), name=\"Softmax_Weights_Matrix\")\n",
    "b = tf.Variable(tf.zeros(shape=(vocab_size,)), name=\"Softmax_Bias_Vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-0f2b29ab14e3>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.add(tf.matmul(embeded, W, name=\"WX\"), b, name=\"WX_plus_b\")\n",
    "\n",
    "#logits = tf.add(tf.matmul(embed, softmax_weights, name=\"WX\"), softmax_bias, name=\"WX_plus_b\")\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_oh, name=\"Compute_Loss\")\n",
    "#mean_loss = tf.reduce_mean(loss)\n",
    "mean_loss = tf.reduce_mean(loss, name=\"Compute_mean_loss\")\n",
    "\n",
    "tf.summary.scalar(\"mean_loss\", mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10000)\n",
      "(32, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (logits.get_shape())\n",
    "print (y_oh.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5, name=\"Optimizer\").minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-aaf4da9003e5>:2: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#compute L2 norm for cosine similarity\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer_1), axis=1, keep_dims=True))\n",
    "normalised_embeddings = embedding_layer_1 / norm\n",
    "\n",
    "# get validation set embeddings\n",
    "validation_data_embeddings = tf.nn.embedding_lookup(normalised_embeddings, ids=valid_X, name=\"validation_embeddings_lookup\")\n",
    "\n",
    "# similarity score of validation embeddings w.r.t normalised= dot product between validation_data_embeddings and mornalised embeddings\n",
    "similarity = tf.matmul(validation_data_embeddings, tf.transpose(normalised_embeddings))  # C.A = C x transpose(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 42816\n",
      "Number of epochs = 20\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 20\n",
    "LOG_DIR = './bigram_wiki_chk_pts'\n",
    "\n",
    "print (\"Number of batches = %d\" %num_batches)\n",
    "print (\"Number of epochs = %d\" %num_of_epochs)\n",
    "\n",
    "\n",
    "validation_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo params \n",
    "\n",
    "num_of_epochs = 5\n",
    "num_batches = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised\n",
      "\n",
      "step_id = 0\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 32.700073\n",
      "\n",
      "\n",
      "For epoch = 0, batch id = 0, batch loss = 32.700073\n",
      "\n",
      "\t Nearest to 44 :  enjoyed, then, revenues, jackson, wishes, scholar, arcade, hospital,\n",
      "\t Nearest to f. :  all, 180, eastern, 2001., remove, lawyers, tourist, province,,\n",
      "\t Nearest to active :  geography, australia's, journal, corps, jews,, function,, imposed, material,\n",
      "\t Nearest to operations, :  issue, afghanistan,, west,, sequence, instrument, lectures, 1952, deep,\n",
      "\t Nearest to gone :  first, adaptation, psychology, compiled, prolific, symmetry, preceded, engineering.,\n",
      "\t Nearest to steam :  canadian, submarine, peoples, prophets, di, 1992., bacteria, schools.,\n",
      "\t Nearest to 1975 :  comoros, pitcher, permitted, aramaic, media., wearing, indeed,, county.,\n",
      "\t Nearest to 1929 :  reacts, chemistry,, position, continuously, pronounced, desert, content, contrasted,\n",
      "step_id = 1\n",
      "step_id = 2\n",
      "step_id = 3\n",
      "step_id = 4\n",
      "step_id = 5\n",
      "step_id = 6\n",
      "step_id = 7\n",
      "step_id = 8\n",
      "step_id = 9\n",
      "step_id = 10\n",
      "step_id = 11\n",
      "step_id = 12\n",
      "step_id = 13\n",
      "step_id = 14\n",
      "step_id = 15\n",
      "step_id = 16\n",
      "step_id = 17\n",
      "step_id = 18\n",
      "step_id = 19\n",
      "step_id = 20\n",
      "step_id = 21\n",
      "step_id = 22\n",
      "step_id = 23\n",
      "step_id = 24\n",
      "step_id = 25\n",
      "step_id = 26\n",
      "step_id = 27\n",
      "step_id = 28\n",
      "step_id = 29\n",
      "step_id = 30\n",
      "step_id = 31\n",
      "step_id = 32\n",
      "step_id = 33\n",
      "step_id = 34\n",
      "step_id = 35\n",
      "step_id = 36\n",
      "step_id = 37\n",
      "step_id = 38\n",
      "step_id = 39\n",
      "step_id = 40\n",
      "step_id = 41\n",
      "step_id = 42\n",
      "step_id = 43\n",
      "step_id = 44\n",
      "step_id = 45\n",
      "step_id = 46\n",
      "step_id = 47\n",
      "step_id = 48\n",
      "step_id = 49\n",
      "\n",
      "For epoch = 0, Av loss = 32.037520\n",
      "step_id = 50\n",
      "\n",
      "For epoch = 1, batch id = 0, batch loss = 32.529381\n",
      "\n",
      "\n",
      "For epoch = 1, batch id = 0, batch loss = 32.529381\n",
      "\n",
      "\t Nearest to 44 :  enjoyed, then, revenues, jackson, wishes, scholar, arcade, hospital,\n",
      "\t Nearest to f. :  all, 180, eastern, 2001., remove, lawyers, tourist, province,,\n",
      "\t Nearest to active :  geography, australia's, journal, corps, jews,, function,, imposed, material,\n",
      "\t Nearest to operations, :  issue, afghanistan,, west,, sequence, instrument, lectures, 1952, deep,\n",
      "\t Nearest to gone :  first, adaptation, psychology, compiled, prolific, symmetry, preceded, minority,\n",
      "\t Nearest to steam :  canadian, submarine, peoples, prophets, di, 1992., bacteria, schools.,\n",
      "\t Nearest to 1975 :  comoros, pitcher, permitted, aramaic, media., wearing, indeed,, county.,\n",
      "\t Nearest to 1929 :  reacts, chemistry,, position, continuously, pronounced, desert, content, contrasted,\n",
      "step_id = 51\n",
      "step_id = 52\n",
      "step_id = 53\n",
      "step_id = 54\n",
      "step_id = 55\n",
      "step_id = 56\n",
      "step_id = 57\n",
      "step_id = 58\n",
      "step_id = 59\n",
      "step_id = 60\n",
      "step_id = 61\n",
      "step_id = 62\n",
      "step_id = 63\n",
      "step_id = 64\n",
      "step_id = 65\n",
      "step_id = 66\n",
      "step_id = 67\n",
      "step_id = 68\n",
      "step_id = 69\n",
      "step_id = 70\n",
      "step_id = 71\n",
      "step_id = 72\n",
      "step_id = 73\n",
      "step_id = 74\n",
      "step_id = 75\n",
      "step_id = 76\n",
      "step_id = 77\n",
      "step_id = 78\n",
      "step_id = 79\n",
      "step_id = 80\n",
      "step_id = 81\n",
      "step_id = 82\n",
      "step_id = 83\n",
      "step_id = 84\n",
      "step_id = 85\n",
      "step_id = 86\n",
      "step_id = 87\n",
      "step_id = 88\n",
      "step_id = 89\n",
      "step_id = 90\n",
      "step_id = 91\n",
      "step_id = 92\n",
      "step_id = 93\n",
      "step_id = 94\n",
      "step_id = 95\n",
      "step_id = 96\n",
      "step_id = 97\n",
      "step_id = 98\n",
      "step_id = 99\n",
      "\n",
      "For epoch = 1, Av loss = 28.675549\n",
      "step_id = 100\n",
      "\n",
      "For epoch = 2, batch id = 0, batch loss = 24.149487\n",
      "\n",
      "\n",
      "For epoch = 2, batch id = 0, batch loss = 24.149487\n",
      "\n",
      "\t Nearest to 44 :  enjoyed, then, revenues, wishes, jackson, arcade, scholar, hospital,\n",
      "\t Nearest to f. :  all, 180, eastern, 2001., remove, lawyers, countries., tourist,\n",
      "\t Nearest to active :  geography, australia's, journal, corps, jews,, function,, imposed, material,\n",
      "\t Nearest to operations, :  issue, afghanistan,, west,, sequence, instrument, lectures, 1952, deep,\n",
      "\t Nearest to gone :  first, adaptation, psychology, compiled, prolific, preceded, symmetry, minority,\n",
      "\t Nearest to steam :  canadian, submarine, peoples, prophets, di, 1992., bacteria, schools.,\n",
      "\t Nearest to 1975 :  comoros, pitcher, permitted, aramaic, media., wearing, indeed,, county.,\n",
      "\t Nearest to 1929 :  reacts, chemistry,, position, continuously, pronounced, content, desert, contrasted,\n",
      "step_id = 101\n",
      "step_id = 102\n",
      "step_id = 103\n",
      "step_id = 104\n",
      "step_id = 105\n",
      "step_id = 106\n",
      "step_id = 107\n",
      "step_id = 108\n",
      "step_id = 109\n",
      "step_id = 110\n",
      "step_id = 111\n",
      "step_id = 112\n",
      "step_id = 113\n",
      "step_id = 114\n",
      "step_id = 115\n",
      "step_id = 116\n",
      "step_id = 117\n",
      "step_id = 118\n",
      "step_id = 119\n",
      "step_id = 120\n",
      "step_id = 121\n",
      "step_id = 122\n",
      "step_id = 123\n",
      "step_id = 124\n",
      "step_id = 125\n",
      "step_id = 126\n",
      "step_id = 127\n",
      "step_id = 128\n",
      "step_id = 129\n",
      "step_id = 130\n",
      "step_id = 131\n",
      "step_id = 132\n",
      "step_id = 133\n",
      "step_id = 134\n",
      "step_id = 135\n",
      "step_id = 136\n",
      "step_id = 137\n",
      "step_id = 138\n",
      "step_id = 139\n",
      "step_id = 140\n",
      "step_id = 141\n",
      "step_id = 142\n",
      "step_id = 143\n",
      "step_id = 144\n",
      "step_id = 145\n",
      "step_id = 146\n",
      "step_id = 147\n",
      "step_id = 148\n",
      "step_id = 149\n",
      "\n",
      "For epoch = 2, Av loss = 26.118314\n",
      "step_id = 150\n",
      "\n",
      "For epoch = 3, batch id = 0, batch loss = 24.611288\n",
      "\n",
      "\n",
      "For epoch = 3, batch id = 0, batch loss = 24.611288\n",
      "\n",
      "\t Nearest to 44 :  enjoyed, then, revenues, arcade, wishes, jackson, scholar, hospital,\n",
      "\t Nearest to f. :  all, 180, eastern, 2001., remove, lawyers, countries., tourist,\n",
      "\t Nearest to active :  geography, australia's, journal, corps, jews,, function,, imposed, material,\n",
      "\t Nearest to operations, :  issue, afghanistan,, west,, sequence, instrument, lectures, 1952, deep,\n",
      "\t Nearest to gone :  first, adaptation, psychology, compiled, prolific, preceded, symmetry, minority,\n",
      "\t Nearest to steam :  canadian, prophets, submarine, di, peoples, 1992., bacteria, schools.,\n",
      "\t Nearest to 1975 :  comoros, pitcher, permitted, aramaic, media., wearing, indeed,, county.,\n",
      "\t Nearest to 1929 :  reacts, chemistry,, position, continuously, pronounced, content, contrasted, reactions,\n",
      "step_id = 151\n",
      "step_id = 152\n",
      "step_id = 153\n",
      "step_id = 154\n",
      "step_id = 155\n",
      "step_id = 156\n",
      "step_id = 157\n",
      "step_id = 158\n",
      "step_id = 159\n",
      "step_id = 160\n",
      "step_id = 161\n",
      "step_id = 162\n",
      "step_id = 163\n",
      "step_id = 164\n",
      "step_id = 165\n",
      "step_id = 166\n",
      "step_id = 167\n",
      "step_id = 168\n",
      "step_id = 169\n",
      "step_id = 170\n",
      "step_id = 171\n",
      "step_id = 172\n",
      "step_id = 173\n",
      "step_id = 174\n",
      "step_id = 175\n",
      "step_id = 176\n",
      "step_id = 177\n",
      "step_id = 178\n",
      "step_id = 179\n",
      "step_id = 180\n",
      "step_id = 181\n",
      "step_id = 182\n",
      "step_id = 183\n",
      "step_id = 184\n",
      "step_id = 185\n",
      "step_id = 186\n",
      "step_id = 187\n",
      "step_id = 188\n",
      "step_id = 189\n",
      "step_id = 190\n",
      "step_id = 191\n",
      "step_id = 192\n",
      "step_id = 193\n",
      "step_id = 194\n",
      "step_id = 195\n",
      "step_id = 196\n",
      "step_id = 197\n",
      "step_id = 198\n",
      "step_id = 199\n",
      "\n",
      "For epoch = 3, Av loss = 25.113707\n",
      "step_id = 200\n",
      "\n",
      "For epoch = 4, batch id = 0, batch loss = 19.190559\n",
      "\n",
      "\n",
      "For epoch = 4, batch id = 0, batch loss = 19.190559\n",
      "\n",
      "\t Nearest to 44 :  enjoyed, then, revenues, arcade, wishes, jackson, scholar, hospital,\n",
      "\t Nearest to f. :  all, 180, eastern, 2001., remove, lawyers, countries., tourist,\n",
      "\t Nearest to active :  geography, australia's, journal, corps, jews,, function,, imposed, material,\n",
      "\t Nearest to operations, :  issue, afghanistan,, west,, sequence, instrument, lectures, 1952, deep,\n",
      "\t Nearest to gone :  first, adaptation, psychology, compiled, prolific, preceded, symmetry, minority,\n",
      "\t Nearest to steam :  canadian, prophets, submarine, di, peoples, 1992., bacteria, schools.,\n",
      "\t Nearest to 1975 :  comoros, pitcher, permitted, aramaic, media., wearing, indeed,, county.,\n",
      "\t Nearest to 1929 :  reacts, chemistry,, position, continuously, pronounced, content, contrasted, reactions,\n",
      "step_id = 201\n",
      "step_id = 202\n",
      "step_id = 203\n",
      "step_id = 204\n",
      "step_id = 205\n",
      "step_id = 206\n",
      "step_id = 207\n",
      "step_id = 208\n",
      "step_id = 209\n",
      "step_id = 210\n",
      "step_id = 211\n",
      "step_id = 212\n",
      "step_id = 213\n",
      "step_id = 214\n",
      "step_id = 215\n",
      "step_id = 216\n",
      "step_id = 217\n",
      "step_id = 218\n",
      "step_id = 219\n",
      "step_id = 220\n",
      "step_id = 221\n",
      "step_id = 222\n",
      "step_id = 223\n",
      "step_id = 224\n",
      "step_id = 225\n",
      "step_id = 226\n",
      "step_id = 227\n",
      "step_id = 228\n",
      "step_id = 229\n",
      "step_id = 230\n",
      "step_id = 231\n",
      "step_id = 232\n",
      "step_id = 233\n",
      "step_id = 234\n",
      "step_id = 235\n",
      "step_id = 236\n",
      "step_id = 237\n",
      "step_id = 238\n",
      "step_id = 239\n",
      "step_id = 240\n",
      "step_id = 241\n",
      "step_id = 242\n",
      "step_id = 243\n",
      "step_id = 244\n",
      "step_id = 245\n",
      "step_id = 246\n",
      "step_id = 247\n",
      "step_id = 248\n",
      "step_id = 249\n",
      "\n",
      "For epoch = 4, Av loss = 24.329157\n",
      "Model saved in file: ./bigram_wiki_chk_pts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# A SIMPLE saver() to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # writer to write graph to tensorboard\n",
    "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    print (\"initialised\\n\")\n",
    "\n",
    "    for epoch_id in range(num_of_epochs):\n",
    "\n",
    "        av_batch_loss = 0\n",
    "\n",
    "        for batch_id in range(num_batches):\n",
    "\n",
    "            X_, Y_ = getNextBatch(bi_grams_=training_data, batch_size=batch_size)\n",
    "\n",
    "            feed_dict = {}\n",
    "            feed_dict[X] = X_\n",
    "            feed_dict[Y] = Y_\n",
    "\n",
    "            batch_loss, _, summary = sess.run([mean_loss, optimizer, summary_op], feed_dict=feed_dict)\n",
    "            \n",
    "            #writer.add_summary(batch_loss, epoch) \n",
    "            step_id = epoch_id * num_batches + batch_id\n",
    "            print (\"step_id = %d\" %step_id)\n",
    "            writer.add_summary(summary, global_step=step_id)\n",
    "\n",
    "            av_batch_loss += batch_loss\n",
    "            \n",
    "            if batch_id % 500 == 0:\n",
    "                print (\"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss))\n",
    "            \n",
    "            if batch_id % 1000 == 0:\n",
    "                print (\"\\nFor epoch = %d, batch id = %d, batch loss = %f\\n\" %(epoch_id, batch_id, batch_loss))\n",
    "                \n",
    "                #print validation data\n",
    "                sim = similarity.eval() # compute similarity\n",
    "                \n",
    "                #iterate over each validation example\n",
    "                \n",
    "                for i in range(validation_size):\n",
    "                    word = idx2word[validation_set[i]]\n",
    "                    top_k = 8\n",
    "                    # sort indexes and pick top k. we take 1:top_k+1 since 0th top pick will the same word itself\n",
    "                    nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    #nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    \n",
    "                    log = '\\t Nearest to %s : ' %word\n",
    "                    for k in range(top_k):\n",
    "                        nearest_word = idx2word[nearest[k]]\n",
    "                        log = '%s %s,' %(log, nearest_word)\n",
    "                    print (log)        \n",
    "\n",
    "        print (\"\\nFor epoch = %d, Av loss = %f\" %(epoch_id, av_batch_loss/num_batches))\n",
    "        \n",
    "        #batch.reset()\n",
    "        \n",
    "    save_path = saver.save(sess, LOG_DIR)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot the Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 5\n",
    "num_batches = 50\n",
    "\n",
    "for epoch_id in range(num_of_epochs):\n",
    "    for batch_id in range(num_batches):\n",
    "        \n",
    "        step_id = epoch_id * num_batches + batch_id\n",
    "        \n",
    "        print (step_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
