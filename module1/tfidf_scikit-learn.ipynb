{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "tfidf_scikit-learn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anujgupta82/NLP_Bootcamp/blob/V_2_0/module1/tfidf_scikit-learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM24wqTW7D4Z",
        "colab_type": "text"
      },
      "source": [
        "Borrowed from [Sebastian Raschka](http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/tfidf_scikit-learn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf5iiuFN7D4c",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIDGtj-F7D4e",
        "colab_type": "text"
      },
      "source": [
        "#Tf-idf Walkthrough for scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOOHZisb7D4f",
        "colab_type": "text"
      },
      "source": [
        "When I was using scikit-learn extremely handy `TfidfVectorizer` I had some trouble interpreting the results since they seem to be a little bit different from the standard convention of how the *term frequency-inverse document frequency* (tf-idf) is calculated. Here, I just put together a brief overview of how the `TfidfVectorizer` works -- mainly as personal reference sheet, but maybe it is useful to one or the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHRjOBVA7D4h",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "### Sections\n",
        "- [What are Tf-idfs all about?](What-are-Tf-idfs-all-about?)\n",
        "- [Example data](#Example-data)\n",
        "- [Raw term frequency](#Raw-term-frequency)\n",
        "- [Normalized term frequency](#Normalized-term-frequency)\n",
        "- [Term frequency-inverse document frequency -- tf-idf](#Term-frequency-inverse-document-frequency----tf-idf)\n",
        "- [Inverse document frequency](#Inverse-document-frequency)\n",
        "- [Normalized tf-idf](#Normalized-tf-idf)\n",
        "- [Smooth idf](#Smooth-idf)\n",
        "- [Tf-idf in scikit-learn](#Tf-idf-in-scikit-learn)\n",
        "- [TfidfVectorizer defaults](#TfidfVectorizer-defaults)\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4K2vgmd7D4i",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2Ebq_47D4m",
        "colab_type": "text"
      },
      "source": [
        "## What are Tf-idfs all about?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n4ArDro7D4p",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MdCKt-Y7D4t",
        "colab_type": "text"
      },
      "source": [
        "Tf-idfs are a way to represent documents as feature vectors. Tf-idfs can be understood as a modification of the *raw term frequencies* (tf); the tf is the count of how often a particular word occurs in a given document. The concept behind the tf-idf is to downweight terms proportionally to the number of documents in which they occur. Here, the idea is that terms that occur in many different documents are likely unimportant or don't contain any useful information for Natural Language Processing tasks such as document classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfC9qS937D4u",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqj3uKY7D4w",
        "colab_type": "text"
      },
      "source": [
        "## Example data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1aP5qWE7D4x",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7X4ZXAg7D4z",
        "colab_type": "text"
      },
      "source": [
        "For the following sections, let us consider the following dataset that consists of 3 documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U2cFkuS7D40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining and the weather is sweet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv33yjZO7D48",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrXEgIRK7D4-",
        "colab_type": "text"
      },
      "source": [
        "## Raw term frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4hGIZT7D5A",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZznUIeL7D5C",
        "colab_type": "text"
      },
      "source": [
        "First, we will start with the *raw term frequency* tf(t, d), which is defined by the  number of times a term *t* occurs in a document *t*\n",
        "\n",
        "<hr>\n",
        "Alternative term frequency definitions include the binary term frequency, log-scaled term frequency, and augmented term frequency [[1](#References)].\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGvow0ok7D5D",
        "colab_type": "text"
      },
      "source": [
        "Using the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn, we can construct a bag-of-words model with the term frequencies as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1KF3AzQ7D5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "91bf5c48-a07d-4dbf-c2ef-bceb8b2d6962"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "tf = cv.fit_transform(docs).toarray()\n",
        "tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 1, 1, 1],\n",
              "       [1, 2, 1, 1, 1, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUvmnzCt7D5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "619a7a42-3801-4744-97c8-501a75c8185a"
      },
      "source": [
        "cv.vocabulary_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0, 'is': 1, 'shining': 2, 'sun': 3, 'sweet': 4, 'the': 5, 'weather': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H8zqGML7D5U",
        "colab_type": "text"
      },
      "source": [
        "Based on the vocabulary, the word \"and\" would be the 1st feature in each document vector in `tf`, the word \"is\" the 2nd, the word \"shining\" the 3rd, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFQd5t467D5W",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUDTs4uq7D5Z",
        "colab_type": "text"
      },
      "source": [
        "## Normalized term frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0oQMv4G7D5a",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkClpV6W7D5c",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will take a brief look at how the tf-feature vector can be normalized, which will be useful later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yV-eMts7D5g",
        "colab_type": "text"
      },
      "source": [
        "The most common way to normalize the raw term frequency is l2-normalization, i.e., dividing the raw term frequency vector $v$ by its length $||v||_2$ (L2- or Euclidean norm).\n",
        "\n",
        "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}} = \\frac{v}{\\big(\\sum_{i=1}^n v_i \\big)^{\\frac{1}{2}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDsODfPd7D5i",
        "colab_type": "text"
      },
      "source": [
        "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTAAxCR27D5j",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{[1, 2, 1, 1, 1, 2, 1]}{2} = [ 0.2773501,  0.5547002,  0.2773501,  0.2773501,  0.2773501,\n",
        "        0.5547002,  0.2773501]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_5kSp667D5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "65843a3d-67c8-427e-fd42-a378fb6e9f29"
      },
      "source": [
        "tf_norm = tf[2] / np.sqrt(np.sum(tf[2]**2))\n",
        "tf_norm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2773501, 0.5547002, 0.2773501, 0.2773501, 0.2773501, 0.5547002,\n",
              "       0.2773501])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-alu06H27D5p",
        "colab_type": "text"
      },
      "source": [
        "In scikit-learn, we can use the [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to normalize the term frequencies if we disable the inverse document frequency calculation (`use_idf: False` and `smooth_idf=False`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-e6RgW47D5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90656cd7-096a-46ea-fe2a-a4f5f912486e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer(use_idf=False, norm='l2', smooth_idf=False)\n",
        "tf_norm = tfidf.fit_transform(tf).toarray()\n",
        "print('Normalized term frequencies of document 3:\\n %s' % tf_norm[-1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized term frequencies of document 3:\n",
            " [0.2773501 0.5547002 0.2773501 0.2773501 0.2773501 0.5547002 0.2773501]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8aOfQPv7D5v",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14I4maxQ7D5x",
        "colab_type": "text"
      },
      "source": [
        "## Term frequency-inverse document frequency -- tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESSYRF1M7D5z",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPu0zciT7D50",
        "colab_type": "text"
      },
      "source": [
        "Most commonly, the term frequency-inverse document frequency (tf-idf) is calculated as follows [[1](#References)]:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUShpRxP7D56",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t),$$\n",
        "where idf is the inverse document frequency, which we will introduce in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvIcrdFt7D57",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfDcNoH47D58",
        "colab_type": "text"
      },
      "source": [
        "## Inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNwsCDfT7D59",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev8rCknQ7D6A",
        "colab_type": "text"
      },
      "source": [
        "In order to understand the *inverse document frequency* idf, let us first introduce the term *document frequency* $\\text{df}(d,t)$, which simply the number of documents $d$ that contain the term $t$. We can then define the idf as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T4kumZW7D6B",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{idf}(t) = log{\\frac{n_d}{1+\\text{df}(d,t)}},$$ \n",
        "where  \n",
        "$n_d$: The total number of documents  \n",
        "$\\text{df}(d,t)$: The number of documents that contain term $t$.\n",
        "\n",
        "Note that the constant 1 is added to the denominator to avoid a zero-division error if a term is not contained in any document in the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD9OsOq-7D6C",
        "colab_type": "text"
      },
      "source": [
        "Now, Let us calculate the idfs of the words \"and\", \"is,\" and \"shining:\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUUcee5b7D6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7c5c2926-9bcf-43c6-d702-3ea9c66c7d69"
      },
      "source": [
        "n_docs = len(docs)\n",
        "\n",
        "df_and = 1\n",
        "idf_and = np.log(n_docs / (1 + df_and))\n",
        "print('idf \"and\": %s' % idf_and)\n",
        "\n",
        "df_is = 3\n",
        "idf_is = np.log(n_docs / (1 + df_is))\n",
        "print('idf \"is\": %s' % idf_is)\n",
        "\n",
        "df_shining = 2\n",
        "idf_shining = np.log(n_docs / (1 + df_shining))\n",
        "print('idf \"shining\": %s' % idf_shining)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idf \"and\": 0.4054651081081644\n",
            "idf \"is\": -0.2876820724517809\n",
            "idf \"shining\": 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf0uDDfB7D6L",
        "colab_type": "text"
      },
      "source": [
        "Using those idfs, we can eventually calculate the tf-idfs for the 3rd document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD5lqyt87D6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a3bd95b8-3458-4573-82b2-a59013d649cc"
      },
      "source": [
        "print('Tf-idfs in document 3:\\n')\n",
        "print('tf-idf \"and\": %s' % (1 * idf_and))\n",
        "print('tf-idf \"is\": %s' % (2 * idf_is))\n",
        "print('tf-idf \"shining\": %s' % (1 * idf_shining))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-idfs in document 3:\n",
            "\n",
            "tf-idf \"and\": 0.4054651081081644\n",
            "tf-idf \"is\": -0.5753641449035618\n",
            "tf-idf \"shining\": 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2gDioG97D6S",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCPmaPZE7D6V",
        "colab_type": "text"
      },
      "source": [
        "## Tf-idf in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ5gUuRL7D6W",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZV4VgY7D6X",
        "colab_type": "text"
      },
      "source": [
        "The tf-idfs in scikit-learn are calculated a little bit differently. Here, the `+1` count is added to the idf, whereas instead of the denominator if the df:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSHbvkIV7D6a",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{idf}(t) = log{\\frac{n_d}{\\text{df}(d,t)}} + 1$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd0WWKKG7D6b",
        "colab_type": "text"
      },
      "source": [
        "We can demonstrate this by calculating the idfs manually using the equation above and comparing the results to the TfidfTransformer output using the settings `use_idf=True, smooth_idf=False, norm=None`.\n",
        "In the following examples, we will be again using the words \"and,\" \"is,\" and \"shining:\" from document 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZidnZfM97D6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de7236f5-9e47-442a-c5d9-705347d2766e"
      },
      "source": [
        "tf_and = 1 \n",
        "df_and = 1 \n",
        "tf_and * (np.log(n_docs / df_and) + 1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.09861228866811"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_0DUJVN7D6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "73ce6816-9af9-4d82-b1a7-309d808ae51a"
      },
      "source": [
        "tf_shining = 2 \n",
        "df_shining = 3 \n",
        "tf_shining * (np.log(n_docs / df_shining) + 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYsjSZq-7D61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7eb9328f-f842-43a3-c808-12935ea86c5c"
      },
      "source": [
        "tf_is = 1 \n",
        "df_is = 2 \n",
        "tf_is * (np.log(n_docs / df_is) + 1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4054651081081644"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7KyHBWj7D67",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2acbd763-e52e-4a9e-cf3b-06f8a23fdec0"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm=None)\n",
        "tfidf.fit_transform(tf).toarray()[-1][0:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.09861229, 2.        , 1.40546511])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMA4T_xs7D7D",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6DnNA-_7D7F",
        "colab_type": "text"
      },
      "source": [
        "## Normalized tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ycta1t7D7G",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYwKY_0V7D7H",
        "colab_type": "text"
      },
      "source": [
        "Now, let us calculate the normalized tf-idfs. Our feature vector of un-normalized tf-idfs for document 3 would look as follows if we'd applied the equation from the previous section to all words in the document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrEFHfxs7D7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idfs_d3 = np.array([[ 2.09861229, 2.0, 1.40546511, 1.40546511, 1.40546511, 2.0, 1.40546511]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzONdU-97D7N",
        "colab_type": "text"
      },
      "source": [
        "Using the l2-norm, we then normalize the tf-idfs as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYQkgie07D7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a9d938d3-2938-41b5-d909-faa02e1c9a24"
      },
      "source": [
        "tf_idfs_d3_norm = tf_idfs_d3[-1] / np.sqrt(np.sum(tf_idfs_d3[-1]**2))\n",
        "tf_idfs_d3_norm"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.46572049, 0.44383662, 0.31189844, 0.31189844, 0.31189844,\n",
              "       0.44383662, 0.31189844])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdVst03M7D7V",
        "colab_type": "text"
      },
      "source": [
        "And finally, we compare the results to the results that the `TfidfTransformer` returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uYpIn3j7D7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ed82b7d1-4f9d-40a9-f8b6-4c6969faf114"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\n",
        "tfidf.fit_transform(tf).toarray()[-1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.46572049, 0.44383662, 0.31189844, 0.31189844, 0.31189844,\n",
              "       0.44383662, 0.31189844])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsSv0OE_7D7a",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N087jLQq7D7c",
        "colab_type": "text"
      },
      "source": [
        "## Smooth idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpI64rC7D7f",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlakvvK7D7g",
        "colab_type": "text"
      },
      "source": [
        "Another parameter in the `TfidfTransformer` is the `smooth_idf`, which is described as\n",
        "> smooth_idf : boolean, default=True  \n",
        "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKGS04Zg7D7i",
        "colab_type": "text"
      },
      "source": [
        "So, our idf would then be defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYLxPi_H7D7n",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqkaV5ub7D7r",
        "colab_type": "text"
      },
      "source": [
        "To confirm that we understand the `smooth_idf` parameter correctly, let us walk through the 3-word example again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQK6VPRo7D7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "95bc9b95-466a-49c4-fca1-a16a5c06ac7d"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm=None)\n",
        "tfidf.fit_transform(tf).toarray()[-1][:3]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.69314718, 2.        , 1.28768207])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6PZl4oO7D77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a442f528-b443-4ff9-9318-553d075f5c9d"
      },
      "source": [
        "tf_and = 1 \n",
        "df_and = 1 \n",
        "tf_and * (np.log((n_docs+1) / (df_and+1)) + 1) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6931471805599454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF5rwgUW7D7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "05e6c17d-4f39-42cf-d16e-be7a60ac82a3"
      },
      "source": [
        "tf_is = 2\n",
        "df_is = 3 \n",
        "tf_is * (np.log((n_docs+1) / (df_is+1)) + 1) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TiRRQ_-7D8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e5a321af-cc82-45aa-aad6-fbacf352f850"
      },
      "source": [
        "tf_shining = 1 \n",
        "df_shining = 2\n",
        "tf_shining * (np.log((n_docs+1) / (df_shining+1)) + 1)  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2876820724517808"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoOLzxLq7D8P",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq9T6Rnw7D8R",
        "colab_type": "text"
      },
      "source": [
        "#TfidfVectorizer defaults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSHdmdML7D8S",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JYIDD-F7D8U",
        "colab_type": "text"
      },
      "source": [
        "Finally, we now understand the default settings in the `TfidfTransformer`, which are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIE21N0-7D8X",
        "colab_type": "text"
      },
      "source": [
        "- `use_idf=True`\n",
        "- `smooth_idf=True`\n",
        "- `norm='l2'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQXcrOrs7D8Y",
        "colab_type": "text"
      },
      "source": [
        "And the equation can be summarized as   \n",
        "$\\text{tf-idf} =  \\text{tf}(t) \\times (\\text{idf}(t, d) + 1),$  \n",
        "where   \n",
        "$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}}.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNYXSylL7D8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e2b0880f-de7c-459a-bf79-1b5563dd31fb"
      },
      "source": [
        "tfidf = TfidfTransformer()\n",
        "tfidf.fit_transform(tf).toarray()[-1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.40474829, 0.47810172, 0.30782151, 0.30782151, 0.30782151,\n",
              "       0.47810172, 0.30782151])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ynZrkB7D8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3dbb5f14-3a89-47b3-ae00-1f255d3d4cb4"
      },
      "source": [
        "smooth_tfidfs_d3 = np.array([[ 1.69314718, 2.0, 1.28768207, 1.28768207, 1.28768207, 2.0, 1.28768207]])\n",
        "smooth_tfidfs_d3_norm = smooth_tfidfs_d3[-1] / np.sqrt(np.sum(smooth_tfidfs_d3[-1]**2))\n",
        "smooth_tfidfs_d3_norm"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.40474829, 0.47810172, 0.30782151, 0.30782151, 0.30782151,\n",
              "       0.47810172, 0.30782151])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVG4j5U7D8f",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA9lVyT47D8m",
        "colab_type": "text"
      },
      "source": [
        "### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHgG7aX7D8p",
        "colab_type": "text"
      },
      "source": [
        "[[back to top](#Sections)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMqKWiy7D8r",
        "colab_type": "text"
      },
      "source": [
        "[1] G. Salton and M. J. McGill. Introduction to modern information retrieval. 1983."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBVpa8gu7D8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdSJZnv7D82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}