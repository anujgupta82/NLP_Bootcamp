{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "LEFTOVERS:   \n",
    "\n",
    "\n",
    "    1) Tensor board graphs for loss    \n",
    "    2) Tensor board for network vis   \n",
    "    3) Is embedding good or not ?\n",
    "    4) Embedding visualization ?\n",
    "    5) model perplexity\n",
    "    \n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='Untitled3_1_LOG.txt', mode='a')\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Harry Potter and the Prisoner of Azkaban\n",
      "\n",
      "by J.K. Rowling\n",
      "\n",
      "CHAPTER ONE\n",
      "\n",
      "OWL POST\n",
      "\n",
      "Harry Potter was a highly unusual boy in many ways. For one thing, he\n",
      "hated the summer holidays more than any other ti\n"
     ]
    }
   ],
   "source": [
    "data_file = '../data/harry_potter_3.txt'\n",
    "\n",
    "fp = open(data_file, 'r')\n",
    "data = fp.read()\n",
    "\n",
    "print (type(data))\n",
    "\n",
    "print (data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 ['1', 'V', '5', 'X', 'z', '6', 'f', '`', 'h', 'w', '_', 'v', '\\\\', \"'\", 'H', 'P', 'S', 'a', 'u', 'K', 'W', '.', 'r', 'O', 'n', 'U', '4', 'd', '?', '7', 'D', 'C', 'T', '-', 'l', 'N', 'L', 'M', 'F', '(', '3', '9', 's', 'p', 'g', 'I', 'A', 'q', 'j', 'x', '0', 't', 'R', ')', '\"', 'G', '2', 'e', ':', '*', 'k', 'Y', 'Q', 'i', 'o', 'B', ' ', '\\n', 'J', '&', 'c', '!', 'y', ',', 'm', ';', 'b', 'E', 'Z']\n"
     ]
    }
   ],
   "source": [
    "#get the character set of data\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print (len(chars), chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 611606 characters, 79 unique.\n"
     ]
    }
   ],
   "source": [
    "data_size, char_set_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, char_set_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7fed07af9fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'ABCDabcd'\n",
    "map(lambda ch: char_to_ix[ch], input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c2i(input):\n",
    "    return map(lambda ch: char_to_ix[ch], input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2c(input):\n",
    "    return map(lambda ch: ix_to_char[ch], input)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------- \n",
    "\n",
    "## Prepare Training data\n",
    "--------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "raw_data = 'abcdefghijklmonpqrstuvwxyzABCDEGHIJKLMONPQRSTUVWXYZ123456789!@#$%^&*()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_len = len(raw_data)\n",
    "print data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "seq_length = 3 # number of steps to unroll the RNN for\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_examples = (data_len - 1) // seq_length\n",
    "print num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_batches = num_examples // batch_size\n",
    "print num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(num_batches):\n",
    "    print \"i = %d\" %i\n",
    "    idx = i * batch_size * seq_length\n",
    "    print \"idx = %d\" %idx\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        start_index = idx + (j*seq_length)\n",
    "        end_index = idx + (j+1)*seq_length\n",
    "        print \"start_index = %d, end_index = %d\" %(start_index, end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_data.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for an epoch, with batches of size batch_size.\n",
    "\n",
    "def gen_epoch_data(raw_data, batch_size, seq_length=25):\n",
    "    \n",
    "    data_len = len(raw_data)\n",
    "    \n",
    "    # num_examples is num_of_sequences. each sequence is an example\n",
    "    num_examples = (data_len - 1) // seq_length\n",
    "    \n",
    "    num_batches = num_examples // batch_size \n",
    "    \n",
    "    # intuituion for the abovce 2 lines is - we are to feed sequences. If each seq is of length k and there are N \n",
    "    # sequences in each batch - then a batch basically consists of k*N characters; \n",
    "    # thus number of batches = data_len // (seq_lenth * batch_size)\n",
    "    \n",
    "    # An easier way to see this is : num of data points = data_len // seq_length\n",
    "    # therefore num of batches = num of data points // batch_size\n",
    "    \n",
    "    # to get training data, we iterate over batches/chunks/rows. First we establish from where the batch data starts \n",
    "    # imagine data to be a a st line ------------- of length data_len\n",
    "    # we split it into batches \n",
    "    #    ----------------\n",
    "    #    ----------------\n",
    "    #    ----------------\n",
    "    \n",
    "    # These (^) text split into chunks/rows - num_of_chunks =  data_len // (seq_lenth * batch_size)\n",
    "    # first get which chuck to pick. This is done by idx = i * batch_size * seq_length\n",
    "    \n",
    "    # once u have the right row - each point in batch is a seq,seq, we need to pick num_of_piars = batch_size\n",
    "    # so iterate over batch_size\n",
    "    # starting point in a chunk is jth sequence * length of 1 seq\n",
    "    \n",
    "    epoch_data = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch = []\n",
    "        idx = i * batch_size * seq_length\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            start_index = idx + (j*seq_length)\n",
    "            end_index = idx + (j+1)*seq_length\n",
    "            input_ = raw_data[start_index : end_index]\n",
    "            target_ = raw_data[start_index + 1 : end_index +1 ]\n",
    "            \n",
    "            batch.append([c2i(input_), c2i(target_)])\n",
    "            \n",
    "        epoch_data.append(batch)\n",
    "        \n",
    "    return epoch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = gen_epoch_data(raw_data=data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 764\n",
      "32\n",
      "[[<map object at 0x7fed07aee5f8>, <map object at 0x7fed07aee668>], [<map object at 0x7fed07aee6d8>, <map object at 0x7fed07aee748>], [<map object at 0x7fed07aee7b8>, <map object at 0x7fed07aee828>], [<map object at 0x7fed07aee908>, <map object at 0x7fed07aee940>], [<map object at 0x7fed07aee9b0>, <map object at 0x7fed07aeea20>], [<map object at 0x7fed07aeea90>, <map object at 0x7fed07aeeb00>], [<map object at 0x7fed07aeeb70>, <map object at 0x7fed07aeebe0>], [<map object at 0x7fed07aeec50>, <map object at 0x7fed07aeecc0>], [<map object at 0x7fed07aeed30>, <map object at 0x7fed07aeeda0>], [<map object at 0x7fed07aeee10>, <map object at 0x7fed07aeee80>], [<map object at 0x7fed07aeeef0>, <map object at 0x7fed07aeef60>], [<map object at 0x7fed07aeefd0>, <map object at 0x7fed07aee518>], [<map object at 0x7fed07af93c8>, <map object at 0x7fed07af9eb8>], [<map object at 0x7fed07af9b70>, <map object at 0x7fed07af9e80>], [<map object at 0x7fed07af9940>, <map object at 0x7fed07af9be0>], [<map object at 0x7fed07af9668>, <map object at 0x7fed07af9c18>], [<map object at 0x7fed07af95f8>, <map object at 0x7fed07aed080>], [<map object at 0x7fed07aed0f0>, <map object at 0x7fed07aed1d0>], [<map object at 0x7fed07aed278>, <map object at 0x7fed07aed2b0>], [<map object at 0x7fed07aed2e8>, <map object at 0x7fed07aed198>], [<map object at 0x7fed07aed3c8>, <map object at 0x7fed07aed400>], [<map object at 0x7fed07aed4a8>, <map object at 0x7fed07aed588>], [<map object at 0x7fed07aed518>, <map object at 0x7fed07aed5f8>], [<map object at 0x7fed07aed668>, <map object at 0x7fed07aed6d8>], [<map object at 0x7fed07aed748>, <map object at 0x7fed07aed7b8>], [<map object at 0x7fed07aed828>, <map object at 0x7fed07aed898>], [<map object at 0x7fed07aed908>, <map object at 0x7fed07aed978>], [<map object at 0x7fed07aed9e8>, <map object at 0x7fed07aeda58>], [<map object at 0x7fed07aedac8>, <map object at 0x7fed07aedb38>], [<map object at 0x7fed07aedba8>, <map object at 0x7fed07aedc18>], [<map object at 0x7fed07aedc88>, <map object at 0x7fed07aedcf8>], [<map object at 0x7fed07aedd68>, <map object at 0x7fed07aeddd8>]]\n"
     ]
    }
   ],
   "source": [
    "print (type(check_data), len(check_data))\n",
    "print (len(check_data[0]))\n",
    "print (check_data[0]) # this is a list of lists of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 2e-1\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "#num_epochs = 500\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.int32, shape=(seq_length), name=\"x\")\n",
    "y = tf.placeholder(tf.int32, shape=(seq_length), name=\"y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot representation of the input\n",
    "x_oh = tf.one_hot(indices=x, depth=vocab_size)\n",
    "y_oh = tf.one_hot(indices=y, depth=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(25,)\n",
      "(25, 79)\n",
      "(25, 79)\n"
     ]
    }
   ],
   "source": [
    "#check and balances \n",
    "\n",
    "print (x.get_shape())\n",
    "print (y.get_shape())\n",
    "\n",
    "print (x_oh.get_shape())\n",
    "print (y_oh.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output as of now is 25 characters, each in 1-hot format of length 80\n",
    "# we will need to feed it 1 character (in 1-hot format) at a time   \n",
    "\n",
    "#We will convert it into desired format in 2 steps - from (25,80) to 25 tensors of\n",
    "#shape (80,) then to 25 tensors of shape (1,80)\n",
    "\n",
    "# Convert the VxN tensor into list of V tensors each of shape = (V,).\n",
    "rnn_inputs = tf.unstack(value=x_oh)\n",
    "rnn_targets = tf.unstack(value=y_oh)\n",
    "\n",
    "#Unpacks the given dimension of a rank=R tensor into rank=(R-1) tensors\n",
    "#i'th tensor in output is the slice value[i, :]. \n",
    "# given a tensor of shape (A, B, C, D);\n",
    "#If axis == 0 (default value) then the i'th tensor in output is the slice value[i, :, :, :] \n",
    "#and return a list of tensors, where each tensor in output will have shape (B, C, D)\n",
    "\n",
    "# for our data, rank of x_oh is 2 and shape = (25,80)\n",
    "#rank is number of dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(79)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_inputs[0].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the weights and biases.\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    \n",
    "    Wxh = tf.Variable(tf.truncated_normal(shape=(hidden_size, vocab_size)), name='Wxh')\n",
    "    Whh = tf.Variable(tf.truncated_normal(shape=(hidden_size, hidden_size)), name='Whh')\n",
    "    Why = tf.Variable(tf.truncated_normal(shape=(vocab_size, hidden_size)), name='Why')    \n",
    "    bh = tf.Variable(tf.zeros(shape=(hidden_size, 1)), name='bh')\n",
    "    by = tf.Variable(tf.zeros(shape=(vocab_size,1)),name='by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rnn_cell(rnn_input, cur_state):\n",
    "    \n",
    "    # expand_dims is required to make the input a 2-D tensor.\n",
    "    # Given a tensor input, this operation inserts a dimension of 1 at the dimension \n",
    "    # index axis of input's shape. The dimension index axis starts at zero;\n",
    "    \n",
    "    with tf.variable_scope('rnn_cell'):\n",
    "        \n",
    "        inp = tf.expand_dims(input=rnn_input, axis=1)\n",
    "\n",
    "        next_state = tf.tanh(tf.matmul(Wxh, inp) + tf.matmul(Whh, cur_state) + bh)\n",
    "        y_hat = tf.matmul(Why, next_state) + by\n",
    "    \n",
    "    return y_hat, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(79), Dimension(1)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "\n",
    "#initialise state\n",
    "state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# Iterate over all the input vectors\n",
    "for rnn_input in rnn_inputs:\n",
    "    y_hat, state = rnn_cell(rnn_input, state)\n",
    "    # Convert y_hat into a 1-D tensor\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    logits.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper method to compute the softmax losses\n",
    "# (It basically compares the outputs to the expected output)\n",
    "for logit, target in zip(logits, rnn_targets):\n",
    "    losses = [tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=target)] \n",
    "              \n",
    "# Compute the average loss over the batch\n",
    "total_loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, the operation below computes the gradients and does the backprop!\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([hidden_size, 1])\n",
    "sample_state = init_state\n",
    "\n",
    "seed = tf.placeholder(tf.int32, [1], name='seed')\n",
    "rnn_input = tf.one_hot(seed, vocab_size)\n",
    "rnn_input = tf.squeeze(rnn_input)\n",
    "\n",
    "y_hat, sample_state = rnn_cell(rnn_input, sample_state)\n",
    "\n",
    "prob = tf.nn.softmax(tf.squeeze(y_hat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network defined, lets train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Epoch: 0 ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4a96895ac56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m#            feed_dict={x:x_i, y:y_i})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_targets_\u001b[0m \u001b[0;34m=\u001b[0m                             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_targets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'map'"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_data = gen_epoch_data(data, batch_size)\n",
    "\n",
    "tlosses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('./Char_Model.log', sess.graph)\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        \n",
    "        print ('--- Starting Epoch:', epoch_idx, '---')\n",
    "        epoch_loss = 0\n",
    "        #epoch_state = np.zeros([hidden_size, 1])\n",
    "        equals = 0.0\n",
    "        \n",
    "        for idx, batch in enumerate(epoch_data):\n",
    "            \n",
    "            training_loss = 0\n",
    "            for example_idx, example in enumerate(batch):\n",
    "                \n",
    "                x_i = example[0]\n",
    "                y_i = example[1]\n",
    "                \n",
    "                #loss, tloss, _, logits_, rnn_targets_, epoch_state = \\\n",
    "                #            sess.run([losses, total_loss, train_step, logits, rnn_targets, state], \\\n",
    "                #            feed_dict={x:x_i, y:y_i})\n",
    "                \n",
    "                loss, tloss, _, logits_, rnn_targets_ = \\\n",
    "                            sess.run([losses, total_loss, optimizer, logits, rnn_targets], \\\n",
    "                            feed_dict={x:x_i, y:y_i})\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                _, tloss,  = sess.run([optimizer, total_loss, logits, rnn_targets], \\\n",
    "                            feed_dict={x:x_i, y:y_i})\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                logits_argmax = np.argmax(logits_, axis=1)\n",
    "                rnn_targets_argmax = np.argmax(rnn_targets_, axis=1)\n",
    "                equals += np.sum(logits_argmax == rnn_targets_argmax)\n",
    "                training_loss += tloss\n",
    "                \n",
    "                if (example_idx % 500 == 0):\n",
    "                    inp_seed = np.array([example[0][0]])\n",
    "\n",
    "                    #print '\\n'\n",
    "                    #print '--- SAMPLE BEGIN ---'\n",
    "                    \n",
    "                    logging.debug(\"\\n\")\n",
    "                    logging.debug(\"--- SAMPLE BEGIN ---\")\n",
    "                    \n",
    "                    num_chars = 100\n",
    "                    ixes = []\n",
    "                    sstate = np.zeros([hidden_size, 1])\n",
    "                    for j in range(num_chars):\n",
    "                        prob_r, sstate = sess.run([prob, sample_state], feed_dict={seed:inp_seed, init_state:sstate, x:x_i})\n",
    "                        ix = np.random.choice(vocab_size, p=prob_r.ravel())\n",
    "                        ixes.append(ix)\n",
    "                        inp_seed = np.array([ix])\n",
    "\n",
    "                    #print ''.join(i2c(ixes))\n",
    "                    #print '--- SAMPLE END ---\\n'\n",
    "                    sent = ''.join(i2c(ixes))\n",
    "                    logging.debug(sent)\n",
    "                    logging.debug(\"--- SAMPLE END ---\\n\")\n",
    "                \n",
    "            training_loss /= len(batch)\n",
    "            equals /= len(batch)\n",
    "            print ('Epoch:', epoch_idx, 'Batch:', idx)\n",
    "            print ('Average training loss in batch:', training_loss)\n",
    "            print ('Average matching chars per batch:', equals)\n",
    "            tlosses.append(training_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e4bc12350aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "tlosses = train()\n",
    "plt.plot(tlosses)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-0341e5202028>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-0341e5202028>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "data_file= '/Users/admin/Documents/Anuj/Coding/Warehouse/input.txt'\n",
    "\n",
    "data = open(data_file, 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 2e-1\n",
    "batch_size = 50\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert an array of chars to array of vocab indices\n",
    "def c2i(inp):\n",
    "    return map(lambda c:char_to_ix[c], inp)\n",
    "\n",
    "def i2c(inp):\n",
    "    return map(lambda c:ix_to_char[c], inp)\n",
    "\n",
    "# Generate data for an epoch, with batches of size batch_size.\n",
    "def gen_epoch_data(raw_data, batch_size):\n",
    "    data_len = len(raw_data)\n",
    "    num_examples = (data_len - 1) // seq_length\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    epoch_data = []\n",
    "    for i in range(num_batches):\n",
    "        batch = []\n",
    "        idx = i * batch_size * seq_length\n",
    "        for j in range(batch_size):\n",
    "            inp = raw_data[idx + j*seq_length:idx + (j+1)*seq_length]\n",
    "            target = raw_data[idx + 1+(j*seq_length):idx + 1+((j+1)*seq_length)]\n",
    "\n",
    "            batch.append([c2i(inp), c2i(target)])\n",
    "        epoch_data.append(batch)\n",
    "    return epoch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'unpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-af52295bec6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrnn_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrnn_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'unpack'"
     ]
    }
   ],
   "source": [
    "epoch_data = gen_epoch_data(data, batch_size)\n",
    "init_state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# Input\n",
    "x = tf.placeholder(tf.int32, shape=(seq_length), name=\"x\")\n",
    "y = tf.placeholder(tf.int32, shape=(seq_length), name=\"y\")\n",
    "state = tf.zeros([hidden_size, 1])\n",
    "\n",
    "# One Hot representation of the input\n",
    "x_oh = tf.one_hot(indices=x, depth=vocab_size)\n",
    "y_oh = tf.one_hot(indices=y, depth=vocab_size)\n",
    "\n",
    "rnn_inputs = tf.unpack(x_oh)\n",
    "rnn_targets = tf.unpack(y_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the weights and biases.\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    Wxh = tf.get_variable('Wxh', [hidden_size, vocab_size])\n",
    "    Whh = tf.get_variable('Whh', [hidden_size, hidden_size])\n",
    "    Why = tf.get_variable('Why', [vocab_size, hidden_size])\n",
    "    bh = tf.get_variable('bh', [hidden_size, 1])\n",
    "    by = tf.get_variable('by', [vocab_size, 1])\n",
    "\n",
    "# Actual math behind computing the output and the next state of the RNN.\n",
    "def rnn_cell(rnn_input, cur_state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        Wxh = tf.get_variable('Wxh', [hidden_size, vocab_size])\n",
    "        Whh = tf.get_variable('Whh', [hidden_size, hidden_size])\n",
    "        Why = tf.get_variable('Why', [vocab_size, hidden_size])\n",
    "        bh = tf.get_variable('bh', [hidden_size, 1])\n",
    "        by = tf.get_variable('by', [vocab_size, 1])\n",
    "    inp = tf.expand_dims(rnn_input, 1)\n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(Wxh, inp) + tf.matmul(Whh, cur_state) + bh)\n",
    "    y_hat = tf.matmul(Why, next_state) + by\n",
    "    return y_hat, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a046b40c2bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadeltaOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-a046b40c2bfb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadeltaOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               instructions)\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1959\u001b[0m   \"\"\"\n\u001b[1;32m   1960\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel, labels,\n\u001b[0;32m-> 1961\u001b[0;31m                     logits)\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits_sg\",\n",
      "\u001b[0;32m~/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1773\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1775\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1776\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "logits = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    y_hat, state = rnn_cell(rnn_input, state)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    logits.append(y_hat)\n",
    "\n",
    "losses = [tf.nn.softmax_cross_entropy_with_logits(logit, target) for logit, target in zip(logits, rnn_targets)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "num_samples = 25\n",
    "sample_state = init_state\n",
    "seed = tf.placeholder(tf.int32, [1], name='seed')\n",
    "rnn_input = tf.one_hot(seed, vocab_size)\n",
    "ixes = []\n",
    "\n",
    "rnn_input = tf.squeeze(rnn_input)\n",
    "y_hat, sample_state = rnn_cell(rnn_input, sample_state)\n",
    "prob = tf.nn.softmax(tf.squeeze(y_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    tlosses = []\n",
    "    \n",
    "    #save the model\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if os.path.isfile(\"model.ckpt\"):\n",
    "            saver.restore(sess, \"model.ckpt\")\n",
    "        else:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            print ('--- Starting Epoch:', epoch_idx, '---')\n",
    "            epoch_loss = 0\n",
    "            epoch_state = np.zeros([hidden_size, 1])\n",
    "            equals = 0.0\n",
    "            for idx, batch in enumerate(epoch_data):\n",
    "                training_loss = 0\n",
    "                for example_idx, example in enumerate(batch):\n",
    "                    x_i = example[0]\n",
    "                    y_i = example[1]\n",
    "\n",
    "                    loss, tloss, _, logits_, rnn_targets_, epoch_state = \\\n",
    "                        sess.run([losses, total_loss, train_step, logits, \\\n",
    "                            rnn_targets, state], \\\n",
    "                                feed_dict={x:x_i, y:y_i, init_state:epoch_state}\n",
    "                        )\n",
    "\n",
    "                    logits_argmax = np.argmax(logits_, axis=1)\n",
    "                    rnn_targets_argmax = np.argmax(rnn_targets_, axis=1)\n",
    "                    equals += np.sum(logits_argmax == rnn_targets_argmax)\n",
    "\n",
    "                    training_loss += tloss\n",
    "\n",
    "                    if (example_idx % 100 == 0):\n",
    "                        inp_seed = np.array([example[0][0]])\n",
    "\n",
    "                        print ('\\n')\n",
    "                        print ('--- SAMPLE BEGIN ---')\n",
    "                        num_chars = 100\n",
    "                        ixes = []\n",
    "                        sstate = np.zeros([hidden_size, 1])\n",
    "                        for j in range(num_chars):\n",
    "                            prob_r, sstate = sess.run([prob, sample_state], feed_dict={seed:inp_seed, init_state:sstate, x:x_i})\n",
    "                            ix = np.random.choice(vocab_size, p=prob_r.ravel())\n",
    "                            ixes.append(ix)\n",
    "                            inp_seed = np.array([ix])\n",
    "\n",
    "                        print (''.join(i2c(ixes)))\n",
    "                        print ('--- SAMPLE END ---')\n",
    "\n",
    "                training_loss /= len(batch)\n",
    "                equals /= len(batch)\n",
    "                print ('Epoch:', epoch_idx, 'Batch:', idx)\n",
    "                print ('Average training loss in batch:', training_loss)\n",
    "                print ('Average matching chars per batch:', equals)\n",
    "                tlosses.append(training_loss)\n",
    "            save_path = saver.save(sess, \"model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "    return tlosses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/piyush/Piyush/study/anthill/anthill/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "--- Starting Epoch: 0 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e4bc12350aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-354c955cb7ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0my_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     loss, tloss, _, logits_, rnn_targets_, epoch_state =                         sess.run([losses, total_loss, train_step, logits,                             rnn_targets, state],                                 feed_dict={x:x_i, y:y_i, init_state:epoch_state}\n\u001b[0m\u001b[1;32m     26\u001b[0m                         )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_step' is not defined"
     ]
    }
   ],
   "source": [
    "tlosses = train()\n",
    "plt.plot(tlosses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
